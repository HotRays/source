From 6208dd2bc8aeb680b24890b14608a8bf67844c16 Mon Sep 17 00:00:00 2001
From: fengmushu <fengmushu@gmail.com>
Date: Mon, 6 Aug 2018 10:31:13 +0800
Subject: [PATCH 2/3] sun6i sun8i dma supported

---
 drivers/bus/Kconfig           |    7 +
 drivers/bus/Makefile          |    1 +
 drivers/bus/sunxi_mbus.c      | 1046 +++++++++++++++++++++++++++
 drivers/dma/Kconfig           |    9 +
 drivers/dma/Makefile          |    1 +
 drivers/dma/sun6i-dma.c       |  186 ++++-
 drivers/dma/sun8i-dma.c       | 1272 +++++++++++++++++++++++++++++++++
 drivers/dma/sunxi-rdma.c      |  973 +++++++++++++++++++++++++
 include/linux/dma/sunxi-dma.h |  237 ------
 include/linux/sunxi-sid.h     |  125 ++++
 include/linux/sunxi-smc.h     |   46 ++
 include/linux/sunxi_mbus.h    |  132 ++++
 12 files changed, 3766 insertions(+), 269 deletions(-)
 create mode 100644 drivers/bus/sunxi_mbus.c
 create mode 100644 drivers/dma/sun8i-dma.c
 create mode 100644 drivers/dma/sunxi-rdma.c
 create mode 100644 include/linux/sunxi-sid.h
 create mode 100644 include/linux/sunxi-smc.h
 create mode 100644 include/linux/sunxi_mbus.h

diff --git a/drivers/bus/Kconfig b/drivers/bus/Kconfig
index 3e66f4cc1..14589d58d 100644
--- a/drivers/bus/Kconfig
+++ b/drivers/bus/Kconfig
@@ -93,6 +93,13 @@ config MVEBU_MBUS
 	  Driver needed for the MBus configuration on Marvell EBU SoCs
 	  (Kirkwood, Dove, Orion5x, MV78XX0 and Armada 370/XP).
 
+config SUNXI_MBUS
+	bool "SUNXI MBUS driver support"
+	depends on ARCH_SUNXI
+	select HWMON
+	help
+	  Driver supporting the mbus for sunxi platforms.
+
 config OMAP_INTERCONNECT
 	tristate "OMAP INTERCONNECT DRIVER"
 	depends on ARCH_OMAP2PLUS
diff --git a/drivers/bus/Makefile b/drivers/bus/Makefile
index 3ae96cffa..d0bee0234 100644
--- a/drivers/bus/Makefile
+++ b/drivers/bus/Makefile
@@ -11,6 +11,7 @@ obj-$(CONFIG_BRCMSTB_GISB_ARB)	+= brcmstb_gisb.o
 obj-$(CONFIG_IMX_WEIM)		+= imx-weim.o
 obj-$(CONFIG_MIPS_CDMM)		+= mips_cdmm.o
 obj-$(CONFIG_MVEBU_MBUS) 	+= mvebu-mbus.o
+obj-$(CONFIG_SUNXI_MBUS)	+= sunxi_mbus.o
 
 # Interconnect bus driver for OMAP SoCs.
 obj-$(CONFIG_OMAP_INTERCONNECT)	+= omap_l3_smx.o omap_l3_noc.o
diff --git a/drivers/bus/sunxi_mbus.c b/drivers/bus/sunxi_mbus.c
new file mode 100644
index 000000000..aac0bc361
--- /dev/null
+++ b/drivers/bus/sunxi_mbus.c
@@ -0,0 +1,1046 @@
+/*
+ * SUNXI MBUS driver
+ *
+ * Copyright (C) 2015 AllWinnertech Ltd.
+ * Author: xiafeng <xiafeng@allwinnertech.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/sunxi_mbus.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of_address.h>
+#include <linux/slab.h>
+#include <linux/hwmon.h>
+#include <linux/hwmon-sysfs.h>
+
+#include <asm/cacheflush.h>
+#include <asm/smp_plat.h>
+
+#define DRIVER_NAME          "MBUS"
+#define DRIVER_NAME_PMU      DRIVER_NAME"_PMU"
+
+#define MBUS_MAST_TMR_REG(n)        (0x000c) /* Time Measurement Register           */
+
+#define MBUS_MAST_CFG0_REG(n)       (0x0210 + (0x10 * n)) /* Master N Configuration Register 0 */
+#define MBUS_MAST_CFG1_REG(n)       (0x0214 + (0x10 * n)) /* Master N Configuration Register 1 */
+#define MBUS_MAST_ABS_BWL_REG(n)    (0x0218 + (0x10 * n)) /* Master N Absolutely Bandwidth Limitation Register */
+
+#define MBUS_BW_CFG_REG             (0x0200) /* Bandwith Window base on MCLK cycles */
+
+#define MBUS_MAST_ACEN_CFG_REG(n)   (0x0020 + (0x04 * n)) /* Master Access Enable, 0:dis, 1:en   */
+
+/* Some platform implement master access priority in register MBUS_MAST_CFG0_REG(n) */
+// #define MBUS_MAST_ACPR_CFG_REG      (0x0098) /* Master Access Priority, 0:low, 1:hg */
+
+#define MBUS_PMU_CNTEB_CFG_REG      (0x009c) /* Counter Enable, 0x0001:enable all   */
+#define MBUS_PMU_CNT_REG(n)         (0x00a0 + (0x4 * n)) /* Counter n = 0 ~ 7       */
+
+#define MBUS_SW_CLK_ON_REG          (0x0030) /* Software Clock ON, 0:open by hws    */
+#define MBUS_SW_CLK_OFF_REG         (0x0040) /* Sofrware Clock OFF, 1:dis-access    */
+
+/* for register MBUS_MAST_CFG0_REG(n) */
+#define MBUS_QOS_MAX            0x03
+#define MBUS_WT_MAX             0x0f  /* wait time, based on MCLK */
+#define MBUS_ACS_MAX            0x0ff /* access commands sequence */
+#define MBUS_BWL_MAX            0x0ffff
+#define MBUS_ABS_BWL_MAX        0x0fff
+#define MBUS_BW_SATU_MAX        0x0fff
+
+#define MBUS_BWLEN_SHIFT        0  /* shirft, Bandwidth Limit function Enable, 0:dis   */
+#define MBUS_PRI_SHIFT          1  /* shirft, Priority, 0:low   */
+#define MBUS_QOS_SHIFT          2  /* shirft, QoS value, 0:lowest, 3:highest           */
+#define MBUS_WT_SHIFT           4  /* shirft, wait time, overflow, pr will be promoted */
+#define MBUS_ACS_SHIFT          8  /* shirft, command number, overflow, CN will be 0   */
+#define MBUS_BWL0_SHIFT         16 /* shirft, Bandwidth Limit in MB/S, 0: no limit     */
+#define MBUS_BWL1_SHIFT         0
+#define MBUS_BWL2_SHIFT         16
+
+#define MBUS_ABS_BWLEN_SHIFT    31
+#define MBUS_ABS_BWL_SHIFT      16
+
+/* for register MBUS_BW_CFG_REG */
+#define MBUS_BWSIZE_MAX         0x0f
+#define MBUS_BWEN_SHIFT         16
+
+/* MBUS PMU ids */
+enum mbus_pmu {
+	MBUS_PMU_CPU    = 0,    /* CPU bandwidth */
+	MBUS_PMU_GPU	= 1,	/* GPU bandwidth */
+	MBUS_PMU_VE	= 2,	/* VE		 */
+	MBUS_PMU_DISP	= 3,	/* DISPLAY	 */
+	MBUS_PMU_OTH    = 4,    /* other masters */
+	MBUS_PMU_TOTAL  = 5,    /* total masters */
+	MBUS_PMU_CSI    = 6,    /* csi masters   */
+	MBUS_PMU_MAX    = 7,    /* max masters   */
+};
+
+#define MBUS_PORT_PRI           (MBUS_PMU_MAX + 0)
+#define MBUS_PORT_QOS           (MBUS_PMU_MAX + 1)
+#define MBUS_PORT_WT            (MBUS_PMU_MAX + 2)
+#define MBUS_PORT_ACS           (MBUS_PMU_MAX + 3)
+#define MBUS_PORT_BWL0          (MBUS_PMU_MAX + 4)
+#define MBUS_PORT_BWL1          (MBUS_PMU_MAX + 5)
+#define MBUS_PORT_BWL2          (MBUS_PMU_MAX + 6)
+#define MBUS_PORT_BWLEN         (MBUS_PMU_MAX + 7)
+#define MBUS_PORT_ABS_BWLEN     (MBUS_PMU_MAX + 8)
+#define MBUS_PORT_ABS_BWL       (MBUS_PMU_MAX + 9)
+#define MBUS_PORT_BW_SATU       (MBUS_PMU_MAX + 10)
+
+struct sunxi_mbus_port {
+	void __iomem *base;
+	unsigned long phys;
+	struct device_node *dn;
+};
+
+static struct sunxi_mbus_port *ports;
+static void __iomem *mbus_ctrl_base;
+static unsigned long mbus_ctrl_phys;
+
+static DEFINE_MUTEX(mbus_seting);
+static DEFINE_MUTEX(mbus_pmureading);
+
+#define mbus_pmu_getstate() \
+	(readl_relaxed(mbus_ctrl_base + MBUS_PMU_CNTEB_CFG_REG) & 1)
+#define mbus_pmu_enable() \
+	writel_relaxed(((readl_relaxed(mbus_ctrl_base + MBUS_PMU_CNTEB_CFG_REG)) | 1), \
+	               mbus_ctrl_base + MBUS_PMU_CNTEB_CFG_REG)
+/**
+ * mbus_port_set_abs_bwlen() - enable a master absolutely bandwidth limit
+ * function
+ *
+ * @port: index of the port to setup
+ * @en: 0-disable, 1-enable
+ */
+int notrace mbus_port_set_abs_bwlen(mbus_port_e port, bool en)
+{
+	unsigned int value;
+
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	mutex_lock(&mbus_seting);
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_ABS_BWL_REG(port));
+	value &= ~(1 << MBUS_ABS_BWLEN_SHIFT);
+	writel_relaxed(value | (en << MBUS_ABS_BWLEN_SHIFT),
+			mbus_ctrl_base + MBUS_MAST_ABS_BWL_REG(port));
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_set_abs_bwlen);
+
+/**
+ * mbus_port_set_abs_bwl() - set a master absolutely bandwidth limit
+ *
+ * @bwl: the number of bandwidth limit
+ */
+int notrace mbus_port_set_abs_bwl(mbus_port_e port, unsigned int bwl)
+{
+	unsigned int value;
+
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	if (bwl > MBUS_ABS_BWL_MAX)
+		return -EPERM;
+
+	mutex_lock(&mbus_seting);
+	/* absolutely bwl, used when en BWL */
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_ABS_BWL_REG(port));
+	value &= ~(MBUS_ABS_BWL_MAX << MBUS_ABS_BWL_SHIFT);
+	writel_relaxed(value | (bwl << MBUS_ABS_BWL_SHIFT),
+			mbus_ctrl_base + MBUS_MAST_ABS_BWL_REG(port));
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_set_abs_bwl);
+
+/**
+ * mbus_port_set_bw_saturation() - set a master bandwidth saturation
+ *
+ * @bw_satu: the number of bandwidth saturation
+ */
+int notrace mbus_port_set_bw_saturation(mbus_port_e port, unsigned int bw_satu)
+{
+	unsigned int value;
+
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	if (bw_satu > MBUS_BW_SATU_MAX)
+		return -EPERM;
+
+	mutex_lock(&mbus_seting);
+	/* absolutely bw_satu, used when en BWL */
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_ABS_BWL_REG(port));
+	value &= ~MBUS_BW_SATU_MAX;
+	writel_relaxed(value | bw_satu,
+		mbus_ctrl_base + MBUS_MAST_ABS_BWL_REG(port));
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_set_bw_saturation);
+
+/**
+ * mbus_port_setreqn() - enable a master bandwidth limit function
+ *
+ * @port: index of the port to setup
+ * @en: 0-disable, 1-enable
+ */
+int notrace mbus_port_setbwlen(mbus_port_e port, bool en)
+{
+	unsigned int value;
+
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	mutex_lock(&mbus_seting);
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+	value &= ~(1 << MBUS_BWLEN_SHIFT);
+	writel_relaxed(value | (en << MBUS_BWLEN_SHIFT), \
+	               mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_setbwlen);
+
+/**
+ * mbus_port_setthd() - set a master priority
+ *
+ * @pri, priority
+ */
+int notrace mbus_port_setpri(mbus_port_e port, bool pri)
+{
+	unsigned int value = 0;
+
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	mutex_lock(&mbus_seting);
+#if (defined MBUS_MAST_ACPR_CFG_REG)
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_ACPR_CFG_REG);
+	value &= ~(1 << port);
+	writel_relaxed(value | (pri << port), \
+			mbus_ctrl_base + MBUS_MAST_ACPR_CFG_REG);
+#else
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+	value &= ~(1 << MBUS_PRI_SHIFT);
+	writel_relaxed(value | (pri << MBUS_PRI_SHIFT), \
+			mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+#endif
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_setpri);
+
+/**
+ * mbus_port_setqos() - set a master QOS
+ *
+ * @qos: the qos value want to set
+ */
+int notrace mbus_port_setqos(mbus_port_e port, unsigned int qos)
+{
+	unsigned int value;
+
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	if (qos > MBUS_QOS_MAX)
+		return -EPERM;
+
+	mutex_lock(&mbus_seting);
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+	value &= ~(MBUS_QOS_MAX << MBUS_QOS_SHIFT);
+	writel_relaxed(value | (qos << MBUS_QOS_SHIFT), \
+	               mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_setqos);
+
+/**
+ * mbus_bw_setbw() - set a master wait time
+ *
+ * @wt: the wait time want to set, based on MCLK
+ */
+int notrace mbus_port_setwt(mbus_port_e port, unsigned int wt)
+{
+	unsigned int value;
+
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	if (wt > MBUS_WT_MAX)
+		return -EPERM;
+
+	mutex_lock(&mbus_seting);
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+	value &= ~(MBUS_WT_MAX << MBUS_WT_SHIFT);
+	writel_relaxed(value | (wt << MBUS_WT_SHIFT), \
+	               mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_setwt);
+
+/**
+ * mbus_bw_setams() - set a master access commands sequence
+ *
+ * @acs: the number of access commands sequency
+ */
+int notrace mbus_port_setacs(mbus_port_e port, unsigned int acs)
+{
+	unsigned int value;
+
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	if (acs > MBUS_ACS_MAX)
+		return -EPERM;
+
+	mutex_lock(&mbus_seting);
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+	value &= ~(MBUS_ACS_MAX << MBUS_ACS_SHIFT);
+	writel_relaxed(value | (acs << MBUS_ACS_SHIFT), \
+	               mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_setacs);
+
+/**
+ * mbus_bw_setbwl0() - function to set bandwidth limit0
+ *
+ * @bwl: the number of bandwidth limit
+ */
+int notrace mbus_port_setbwl0(mbus_port_e port, unsigned int bwl0)
+{
+	unsigned int value;
+
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	if (bwl0 > MBUS_BWL_MAX)
+		return -EPERM;
+
+	mutex_lock(&mbus_seting);
+	/* bwl0, used when BWL function enable */
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+	value &= ~(MBUS_BWL_MAX << MBUS_BWL0_SHIFT);
+	writel_relaxed(value | (bwl0 << MBUS_BWL0_SHIFT), \
+	               mbus_ctrl_base + MBUS_MAST_CFG0_REG(port));
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_setbwl0);
+
+/**
+ * mbus_bw_setbwl1() - set a master bandwidth limit1
+ *
+ * @bwl: the number of bandwidth limit
+ */
+int notrace mbus_port_setbwl1(mbus_port_e port, unsigned int bwl1)
+{
+	unsigned int value;
+
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	if (bwl1 > MBUS_BWL_MAX)
+		return -EPERM;
+
+	mutex_lock(&mbus_seting);
+	/* bwl1, used when en BWL */
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_CFG1_REG(port));
+	value &= ~(MBUS_BWL_MAX << MBUS_BWL1_SHIFT);
+	writel_relaxed(value | (bwl1 << MBUS_BWL1_SHIFT), \
+	               mbus_ctrl_base + MBUS_MAST_CFG1_REG(port));
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_setbwl1);
+
+/**
+ * mbus_bw_setbwl2() - set a master bandwidth limit2
+ *
+ * @bwl: the number of bandwidth limit
+ */
+int notrace mbus_port_setbwl2(mbus_port_e port, unsigned int bwl2)
+{
+	unsigned int value;
+
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	if (bwl2 > MBUS_BWL_MAX)
+		return -EPERM;
+
+	mutex_lock(&mbus_seting);
+	/* bwl2, used when en BWL */
+	value = readl_relaxed(mbus_ctrl_base + MBUS_MAST_CFG1_REG(port));
+	value &= ~(MBUS_BWL_MAX << MBUS_BWL2_SHIFT);
+	writel_relaxed(value | (bwl2 << MBUS_BWL2_SHIFT), \
+	               mbus_ctrl_base + MBUS_MAST_CFG1_REG(port));
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_setbwl2);
+
+/**
+ * mbus_bw_setbwl() - set a master bandwidth limit
+ *
+ * @bwl0/1/2: the number of bandwidth limit0/1/2
+ */
+int notrace mbus_port_setbwl(mbus_port_e port, unsigned int bwl0, \
+                             unsigned int bwl1, unsigned int bwl2)
+{
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+
+	if ((bwl0 > MBUS_BWL_MAX) || (bwl1 > MBUS_BWL_MAX) || \
+	    (bwl2 > MBUS_BWL_MAX))
+		return -EPERM;
+
+	mbus_port_setbwl0(port, bwl0);
+	mbus_port_setbwl1(port, bwl1);
+	mbus_port_setbwl2(port, bwl2);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_setbwl);
+
+/**
+ * mbus_bw_control() - set BandWidth limit window enable or disable
+ *
+ * @enable: if true enables the bwlw, if false disables it
+ */
+int notrace mbus_set_bwlwen(bool enable)
+{
+	unsigned int value;
+
+	mutex_lock(&mbus_seting);
+	value = readl_relaxed(mbus_ctrl_base + MBUS_BW_CFG_REG);
+
+	writel_relaxed(enable ? (value | (1 << MBUS_BWEN_SHIFT)) : \
+	               (value & ~(1 << MBUS_BWEN_SHIFT)), \
+	               mbus_ctrl_base + MBUS_BW_CFG_REG);
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_set_bwlwen);
+
+/**
+ * mbus_bw_control() - set BandWidth limit window size
+ *
+ * @size: the size of bwl window, base on MCLK
+ */
+int notrace mbus_set_bwlwsiz(unsigned int size)
+{
+	unsigned int value;
+
+	mutex_lock(&mbus_seting);
+	value = readl_relaxed(mbus_ctrl_base + MBUS_BW_CFG_REG);
+	value &= ~MBUS_BWSIZE_MAX;
+	writel_relaxed(value | size, mbus_ctrl_base + MBUS_BW_CFG_REG);
+	mutex_unlock(&mbus_seting);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_set_bwlwsiz);
+
+/**
+ * They are called by low-level power management code to disable slave
+ * interfaces snoops and DVM broadcast.
+ */
+
+/**
+ * mbus_port_control() - control a master port access DRAM
+ *
+ * @enable: if true enables the port, if false disables it
+ */
+static void notrace mbus_port_control(mbus_port_e port, bool enable)
+{
+	unsigned int value, reg, pos;
+
+	reg = (port <= 31) ? MBUS_MAST_ACEN_CFG_REG(0) : MBUS_MAST_ACEN_CFG_REG(1);
+	pos = (port <= 31) ? port : (port - 31);
+
+	/*
+	 * This function is called from power down procedures
+	 * and must not execute any instruction that might
+	 * cause the processor to be put in a quiescent state
+	 * (eg wfi). Hence, cpu_relax() can not be added to this
+	 * read loop to optimize power, since it might hide possibly
+	 * disruptive operations.
+	 */
+	mutex_lock(&mbus_seting);
+	value = readl_relaxed(mbus_ctrl_base + reg);
+	if (enable) {
+		value |= (1 << pos);
+	} else {
+		value &= ~(1 << pos);
+	}
+	writel_relaxed(value, mbus_ctrl_base + reg);
+	mutex_unlock(&mbus_seting);
+}
+
+/**
+ * mbus_control_port_by_index() - control a master port by port index
+ *
+ * @port: port index previously retrieved with mbus_ace_get_port()
+ * @enable: if true enables the port, if false disables it
+ *
+ * Return:
+ *	0 on success
+ *	-ENODEV on port index out of range
+ *	-EPERM if operation carried out on an ACE PORT
+ */
+int notrace mbus_port_control_by_index(mbus_port_e port, bool enable)
+{
+	if (port >= MBUS_PORTS_MAX)
+		return -ENODEV;
+	/*
+	 * MBUS control for ports connected to CPUS is extremely fragile
+	 * and must be made to go through a specific.
+	 */
+
+	mbus_port_control(port, enable);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mbus_port_control_by_index);
+
+static const struct of_device_id sunxi_mbus_matches[] = {
+	{.compatible = "allwinner,sun8i-mbus", },
+	{.compatible = "allwinner,sun50i-mbus", },
+	{},
+};
+
+static int mbus_probe(void)
+{
+	int ret;
+	struct device_node *np;
+	struct resource res;
+
+	np = of_find_matching_node(NULL, sunxi_mbus_matches);
+	if (!np)
+		return -ENODEV;
+
+	ports = kcalloc(sizeof(*ports), 1, GFP_KERNEL);
+	if (!ports)
+		return -ENOMEM;
+
+	ret = of_address_to_resource(np, 0, &res);
+	if (!ret) {
+		mbus_ctrl_base = ioremap(res.start, resource_size(&res));
+		mbus_ctrl_phys = res.start;
+
+	}
+	if (ret || !mbus_ctrl_base) {
+		WARN(1, "unable to ioremap mbus ctrl\n");
+		ret = -ENXIO;
+		goto memalloc_err;
+	}
+
+	/* the purpose freq of MBUS is 400M, has been configied by boot */
+
+	/* all the port is default opened */
+
+	/* set default bandwidth */
+
+	/* set default QOS */
+
+	/* set masters' request number sequency */
+
+	/* set masters' bandwidth limit0/1/2 */
+
+	//sync_cache_w(&mbus_ctrl_base);
+	//sync_cache_w(&mbus_ctrl_phys);
+	//sync_cache_w(&ports);
+	//__sync_cache_range_w(ports, sizeof(*ports));
+
+memalloc_err:
+	kfree(ports);
+
+	return 0;
+}
+
+static int mbus_init_status = -EAGAIN;
+static DEFINE_MUTEX(mbus_proing);
+
+static int mbus_init(void)
+{
+	if (mbus_init_status != -EAGAIN)
+		return mbus_init_status;
+
+	mutex_lock(&mbus_proing);
+	if (mbus_init_status == -EAGAIN)
+		mbus_init_status = mbus_probe();
+	mutex_unlock(&mbus_proing);
+
+	return mbus_init_status;
+}
+
+/**
+ * To sort out early init calls ordering a helper function is provided to
+ * check if the mbus driver has beed initialized. Function check if the driver
+ * has been initialized, if not it calls the init function that probes
+ * the driver and updates the return value.
+ */
+bool mbus_probed(void)
+{
+	return mbus_init() == 0;
+}
+EXPORT_SYMBOL_GPL(mbus_probed);
+
+struct mbus_data {
+	struct device *hwmon_dev;
+	struct mutex update_lock;
+	bool valid;
+	unsigned long last_updated;
+	int kind;
+};
+
+static struct mbus_data hw_mbus_pmu;
+
+static unsigned int mbus_update_device(struct mbus_data *data,
+				       enum mbus_pmu port)
+{
+	unsigned int value = 0;
+
+	mutex_lock(&data->update_lock);
+
+	/* confirm the pmu is enabled */
+	if(!mbus_pmu_getstate())
+		mbus_pmu_enable();
+
+	/* read pmu conter */
+	value = readl_relaxed(mbus_ctrl_base + MBUS_PMU_CNT_REG(port));
+
+	mutex_unlock(&data->update_lock);
+
+	return value;
+}
+
+#define for_each_ports(port) for (port = 0; port < MBUS_PORTS_MAX; port++)
+
+static unsigned int mbus_get_value(struct mbus_data *data, \
+                                   unsigned int index, char *buf)
+{
+	unsigned int i, size = 0;
+	unsigned int value;
+
+	mutex_lock(&data->update_lock);
+	switch (index) {
+	case MBUS_PORT_PRI:
+		for_each_ports(i) {
+			value = readl_relaxed(mbus_ctrl_base + \
+			                      MBUS_MAST_CFG0_REG(i));
+			value >>= MBUS_PRI_SHIFT;
+			size += sprintf(buf + size, "master%2d " \
+			                "priority:%1d\n", i, (value & 1));
+			value >>= 1;
+		}
+		break;
+	case MBUS_PORT_QOS:
+		for_each_ports(i) {
+			value = readl_relaxed(mbus_ctrl_base + \
+			                      MBUS_MAST_CFG0_REG(i));
+			value >>= MBUS_QOS_SHIFT;
+			value &= MBUS_QOS_MAX;
+			size += sprintf(buf + size, "master%2d qos:%1d\n", \
+			                i, value);
+		}
+		break;
+	case MBUS_PORT_WT:
+		for_each_ports(i) {
+			value = readl_relaxed(mbus_ctrl_base + \
+			                      MBUS_MAST_CFG0_REG(i));
+			value >>= MBUS_WT_SHIFT;
+			value &= MBUS_WT_MAX;
+			size += sprintf(buf + size, "master%2d " \
+			                "threshold0:%2d\n", i, value);
+		}
+		break;
+	case MBUS_PORT_ACS:
+		for_each_ports(i) {
+			value = readl_relaxed(mbus_ctrl_base + \
+			                      MBUS_MAST_CFG0_REG(i));
+			value >>= MBUS_ACS_SHIFT;
+			value &= MBUS_ACS_MAX;
+			size += sprintf(buf + size, "master%2d accsess " \
+			                "commands:%4d\n", i, value);
+		}
+		break;
+	case MBUS_PORT_BWL0:
+		 for_each_ports(i) {
+			value = readl_relaxed(mbus_ctrl_base + \
+			                      MBUS_MAST_CFG0_REG(i));
+			value >>= MBUS_BWL0_SHIFT;
+			value &= MBUS_BWL_MAX;
+			size += sprintf(buf + size, "master%2d bandwidth " \
+			                "limit0:%5d\n", i, value);
+		}
+		break;
+	case MBUS_PORT_BWL1:
+		for_each_ports(i) {
+			value = readl_relaxed(mbus_ctrl_base + \
+			                      MBUS_MAST_CFG1_REG(i));
+			value >>= MBUS_BWL1_SHIFT;
+			value &= MBUS_BWL_MAX;
+			size += sprintf(buf + size, "master%2d bandwidth " \
+			                "limit1:%5d\n", i, value);
+		}
+		break;
+	case MBUS_PORT_BWL2:
+		for_each_ports(i) {
+			value = readl_relaxed(mbus_ctrl_base + \
+			                      MBUS_MAST_CFG1_REG(i));
+			value >>= MBUS_BWL2_SHIFT;
+			value &= MBUS_BWL_MAX;
+			size += sprintf(buf + size, "master%2d bandwidth " \
+			                "limit2:%5d\n", i, value);
+		}
+		break;
+	case MBUS_PORT_BWLEN:
+		for_each_ports(i) {
+			value = readl_relaxed(mbus_ctrl_base + \
+			                      MBUS_MAST_CFG0_REG(i));
+			value &= 1;
+			size += sprintf(buf + size, "master%2d " \
+			                "BWLimit_en:%1d\n", i, value);
+		}
+		break;
+	case MBUS_PORT_ABS_BWLEN:
+		for_each_ports(i) {
+			value = readl_relaxed(mbus_ctrl_base +
+					MBUS_MAST_ABS_BWL_REG(i));
+			value >>= MBUS_ABS_BWLEN_SHIFT;
+			value &= 1;
+			size += sprintf(buf + size,
+			"master%2d absolutely BWLimit_en:%1d\n", i, value);
+		}
+		break;
+	case MBUS_PORT_ABS_BWL:
+		for_each_ports(i) {
+			value = readl_relaxed(mbus_ctrl_base +
+					MBUS_MAST_ABS_BWL_REG(i));
+			value >>= MBUS_ABS_BWL_SHIFT;
+			value &= MBUS_ABS_BWL_MAX;
+			size += sprintf(buf + size,
+			"master%2d absolutely bandwidth limit:%5d\n", i, value);
+		}
+		break;
+	case MBUS_PORT_BW_SATU:
+		for_each_ports(i) {
+			value = readl_relaxed(mbus_ctrl_base +
+					MBUS_MAST_ABS_BWL_REG(i));
+			value &= MBUS_BW_SATU_MAX;
+			size += sprintf(buf + size,
+			"master%2d bandwidth saturation:%5d\n", i, value);
+		}
+		break;
+	default:
+		/* programmer goofed */
+		WARN_ON_ONCE(1);
+		value = 0;
+		break;
+	}
+	mutex_unlock(&data->update_lock);
+
+	return size;
+}
+
+static ssize_t mbus_show_value(struct device *dev, \
+                               struct device_attribute *da, char *buf)
+{
+	struct sensor_device_attribute *attr = to_sensor_dev_attr(da);
+	unsigned int len;
+
+	if (attr->index >= MBUS_PMU_MAX) {
+		len = mbus_get_value(&hw_mbus_pmu, attr->index, buf);
+		len = (len < PAGE_SIZE) ? len : PAGE_SIZE;
+		return len;
+	}
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", \
+			mbus_update_device(&hw_mbus_pmu, attr->index));
+}
+
+static unsigned int mbus_set_value(struct mbus_data *data, unsigned int index,\
+                                   mbus_port_e port, unsigned int val)
+{
+	unsigned int value;
+
+	mutex_lock(&data->update_lock);
+	switch (index) {
+	case MBUS_PORT_PRI:
+		mbus_port_setpri(port, val);
+		break;
+	case MBUS_PORT_QOS:
+		mbus_port_setqos(port, val);
+		break;
+	case MBUS_PORT_WT:
+		mbus_port_setwt(port, val);
+		break;
+	case MBUS_PORT_ACS:
+		mbus_port_setacs(port, val);
+		break;
+	case MBUS_PORT_BWL0:
+		mbus_port_setbwl0(port, val);
+		break;
+	case MBUS_PORT_BWL1:
+		mbus_port_setbwl1(port, val);
+		break;
+	case MBUS_PORT_BWL2:
+		mbus_port_setbwl2(port, val);
+		break;
+	case MBUS_PORT_BWLEN:
+		mbus_port_setbwlen(port, val);
+		break;
+	case MBUS_PORT_ABS_BWLEN:
+		mbus_port_set_abs_bwlen(port, val);
+		break;
+	case MBUS_PORT_ABS_BWL:
+		mbus_port_set_abs_bwl(port, val);
+		break;
+	case MBUS_PORT_BW_SATU:
+		mbus_port_set_bw_saturation(port, val);
+		break;
+	default:
+		/* programmer goofed */
+		WARN_ON_ONCE(1);
+		value = 0;
+		break;
+	}
+	mutex_unlock(&data->update_lock);
+
+	return 0;
+}
+
+static ssize_t mbus_store_value(struct device *dev, \
+                                struct device_attribute *attr, \
+                                const char *buf, size_t count)
+{
+	int nr = to_sensor_dev_attr(attr)->index;
+	unsigned long port, val;
+	unsigned char buffer[64];
+	unsigned char *pbuf, *pbufi;
+	int err;
+
+	if (strlen(buf) >= 64) {
+		dev_err(dev, "arguments out of range!\n");
+		return -EINVAL;
+	}
+
+	while(*buf == ' ') /* find the first unblank character */
+		buf++;
+	strncpy(buffer, buf, strlen(buf));
+
+	pbufi = buffer;
+	while(*pbufi != ' ') /* find the first argument */
+		pbufi++;
+	*pbufi = 0x0;
+	pbuf = (unsigned char *)buffer;
+	err = kstrtoul(pbuf, 10, &port);
+	if (err < 0)
+		return err;
+	if (port >= MBUS_PORTS_MAX) {
+		dev_err(dev, "master is illegal\n");
+		return -EINVAL;
+	}
+
+	pbuf = ++pbufi;
+	while(*pbuf == ' ') /* remove extra space character */
+		pbuf++;
+	pbufi = pbuf;
+	while((*pbufi != ' ') && (*pbufi != '\n'))
+		pbufi++;
+	*pbufi = 0x0;
+
+	err = kstrtoul(pbuf, 10, &val);
+	if (err < 0)
+		return err;
+
+	mbus_set_value(&hw_mbus_pmu, nr, (mbus_port_e)port, (unsigned int)val);
+
+	return count;
+}
+
+/* CPU bandwidth of DDR channel 0 */
+static SENSOR_DEVICE_ATTR(pmu_cpuddr, S_IRUGO, mbus_show_value, NULL, MBUS_PMU_CPU);
+
+/* GPU bandwidth of DDR channel 0 */
+static SENSOR_DEVICE_ATTR(pmu_gpuddr, S_IRUGO, mbus_show_value, NULL, MBUS_PMU_GPU);
+/* VE & CSI & FD bandwidth of DDR channel 0 */
+static SENSOR_DEVICE_ATTR(pmu_ve_ddr, S_IRUGO, mbus_show_value, NULL, MBUS_PMU_VE);
+/* DE bandwidth of DDR channel 0 */
+static SENSOR_DEVICE_ATTR(pmu_de_ddr, S_IRUGO, mbus_show_value, NULL, MBUS_PMU_DISP);
+
+/* other master bandwidth of DDR channel 0 */
+static SENSOR_DEVICE_ATTR(pmu_othddr, S_IRUGO, mbus_show_value, NULL, MBUS_PMU_OTH);
+/* total bandwidth of DDR channel 0 */
+static SENSOR_DEVICE_ATTR(pmu_totddr, S_IRUGO, mbus_show_value, NULL, MBUS_PMU_TOTAL);
+
+/* csi bandwidth of CSI channel 0 */
+static SENSOR_DEVICE_ATTR(pmu_csiddr, S_IRUGO, mbus_show_value, NULL, MBUS_PMU_CSI);
+
+/* get all masters' priority or set a master's priority */
+static SENSOR_DEVICE_ATTR(port_prio, S_IRUGO | S_IWUSR, mbus_show_value, mbus_store_value, MBUS_PORT_PRI);
+/* get all masterss' qos or set a master's qos */
+static SENSOR_DEVICE_ATTR(port_qos, S_IRUGO | S_IWUSR, mbus_show_value, mbus_store_value, MBUS_PORT_QOS);
+/* get all masterss' threshold or set a master's threshold */
+static SENSOR_DEVICE_ATTR(port_watt, S_IRUGO | S_IWUSR, mbus_show_value, mbus_store_value, MBUS_PORT_WT);
+/* get all masterss' threshold or set a master's threshold */
+static SENSOR_DEVICE_ATTR(port_acs, S_IRUGO | S_IWUSR, mbus_show_value, mbus_store_value, MBUS_PORT_ACS);
+/* get all masters' requeset number or set a master's number */
+static SENSOR_DEVICE_ATTR(port_bwl0, S_IRUGO | S_IWUSR, mbus_show_value, mbus_store_value, MBUS_PORT_BWL0);
+/* get all masters' requeset number or set a master's number */
+static SENSOR_DEVICE_ATTR(port_bwl1, S_IRUGO | S_IWUSR, mbus_show_value, mbus_store_value, MBUS_PORT_BWL1);
+/* get all masters' requeset number or set a master's number */
+static SENSOR_DEVICE_ATTR(port_bwl2, S_IRUGO | S_IWUSR, mbus_show_value, mbus_store_value, MBUS_PORT_BWL2);
+/* get all masters' requeset number or set a master's number */
+static SENSOR_DEVICE_ATTR(port_bwlen, S_IRUGO | S_IWUSR, mbus_show_value, mbus_store_value, MBUS_PORT_BWLEN);
+/* get all masters' requeset number or set a master's number */
+static SENSOR_DEVICE_ATTR(port_abs_bwlen, S_IRUGO | S_IWUSR, mbus_show_value, mbus_store_value, MBUS_PORT_ABS_BWLEN);
+/* get all masters' requeset number or set a master's number */
+static SENSOR_DEVICE_ATTR(port_abs_bwl, S_IRUGO | S_IWUSR, mbus_show_value, mbus_store_value, MBUS_PORT_ABS_BWL);
+/* get all masters' requeset number or set a master's number */
+static SENSOR_DEVICE_ATTR(port_bw_satu, S_IRUGO | S_IWUSR, mbus_show_value, mbus_store_value, MBUS_PORT_BW_SATU);
+
+/* pointers to created device attributes */
+static struct attribute *mbus_attributes[] = {
+	&sensor_dev_attr_pmu_cpuddr.dev_attr.attr,
+	&sensor_dev_attr_pmu_gpuddr.dev_attr.attr,
+	&sensor_dev_attr_pmu_ve_ddr.dev_attr.attr,
+	&sensor_dev_attr_pmu_de_ddr.dev_attr.attr,
+	&sensor_dev_attr_pmu_othddr.dev_attr.attr,
+	&sensor_dev_attr_pmu_totddr.dev_attr.attr,
+	&sensor_dev_attr_pmu_csiddr.dev_attr.attr,
+	&sensor_dev_attr_port_prio.dev_attr.attr,
+	&sensor_dev_attr_port_qos.dev_attr.attr,
+	&sensor_dev_attr_port_watt.dev_attr.attr,
+	&sensor_dev_attr_port_acs.dev_attr.attr,
+	&sensor_dev_attr_port_bwl0.dev_attr.attr,
+	&sensor_dev_attr_port_bwl1.dev_attr.attr,
+	&sensor_dev_attr_port_bwl2.dev_attr.attr,
+	&sensor_dev_attr_port_bwlen.dev_attr.attr,
+	&sensor_dev_attr_port_abs_bwlen.dev_attr.attr,
+	&sensor_dev_attr_port_abs_bwl.dev_attr.attr,
+	&sensor_dev_attr_port_bw_satu.dev_attr.attr,
+	NULL,
+};
+
+static const struct attribute_group mbus_group = {
+	.attrs = mbus_attributes,
+};
+__ATTRIBUTE_GROUPS(mbus);
+
+static int mbus_pmu_probe(struct platform_device *pdev)
+{
+	int ret;
+
+	hw_mbus_pmu.hwmon_dev = hwmon_device_register_with_groups(&pdev->dev,
+				"mbus_pmu", dev_get_drvdata(&pdev->dev), mbus_groups);
+	if (IS_ERR(hw_mbus_pmu.hwmon_dev)) {
+		ret = PTR_ERR(hw_mbus_pmu.hwmon_dev);
+		goto out_err;
+	}
+
+	hw_mbus_pmu.last_updated = 0;
+	hw_mbus_pmu.valid = 0;
+	mutex_init(&hw_mbus_pmu.update_lock);
+
+	return 0;
+
+out_err:
+	dev_err(&(pdev->dev), "probed faild\n");
+	return ret;
+}
+
+static int mbus_pmu_remove(struct platform_device *pdev)
+{
+	hwmon_device_unregister(hw_mbus_pmu.hwmon_dev);
+	return 0;
+}
+
+#ifdef CONFIG_PM
+static int sunxi_mbus_suspend(struct device *dev)
+{
+	dev_info(dev, "suspend okay\n");
+
+	return 0;
+}
+
+static int sunxi_mbus_resume(struct device *dev)
+{
+	dev_info(dev, "resume okay\n");
+
+	return 0;
+}
+
+static const struct dev_pm_ops sunxi_mbus_pm_ops = {
+	.suspend = sunxi_mbus_suspend,
+	.resume = sunxi_mbus_resume,
+};
+
+#define SUNXI_MBUS_PM_OPS (&sunxi_mbus_pm_ops)
+#else
+#define SUNXI_MBUS_PM_OPS NULL
+#endif
+
+static struct platform_driver mbus_pmu_driver = {
+	.driver = {
+		.name   = DRIVER_NAME_PMU,
+		.owner  = THIS_MODULE,
+		.pm     = SUNXI_MBUS_PM_OPS,
+		.of_match_table = sunxi_mbus_matches,
+	},
+	.probe = mbus_pmu_probe,
+	.remove = mbus_pmu_remove,
+};
+
+static int __init mbus_pmu_init(void)
+{
+	int ret;
+
+	ret = platform_driver_register(&mbus_pmu_driver);
+	if (ret) {
+		pr_err("register sunxi mbus platform driver failed\n");
+		goto drv_err;
+	}
+
+	return ret;
+
+drv_err:
+	platform_driver_unregister(&mbus_pmu_driver);
+
+	return -EINVAL;
+}
+
+early_initcall(mbus_init);
+device_initcall(mbus_pmu_init);
+
+MODULE_LICENSE("GPL V2");
+MODULE_DESCRIPTION("SUNXI MBUS support");
+MODULE_AUTHOR("xiafeng");
diff --git a/drivers/dma/Kconfig b/drivers/dma/Kconfig
index fadc4d878..beab9865b 100644
--- a/drivers/dma/Kconfig
+++ b/drivers/dma/Kconfig
@@ -187,6 +187,15 @@ config DMA_SUN6I
 	help
 	  Support for the DMA engine first found in Allwinner A31 SoCs.
 
+config DMA_SUN8I
+	tristate "Allwinner 8I SoCs DMA support"
+	depends on ARCH_SUNXI || COMPILE_TEST
+	depends on RESET_CONTROLLER
+	select DMA_ENGINE
+	select DMA_VIRTUAL_CHANNELS
+	help
+	  Support for the DMA engine first found in Allwinner 8I SoCs.
+
 config EP93XX_DMA
 	bool "Cirrus Logic EP93xx DMA support"
 	depends on ARCH_EP93XX || COMPILE_TEST
diff --git a/drivers/dma/Makefile b/drivers/dma/Makefile
index 9d0156b50..4ad0186d1 100644
--- a/drivers/dma/Makefile
+++ b/drivers/dma/Makefile
@@ -28,6 +28,7 @@ obj-$(CONFIG_DMA_OMAP) += omap-dma.o
 obj-$(CONFIG_DMA_SA11X0) += sa11x0-dma.o
 obj-$(CONFIG_DMA_SUN4I) += sun4i-dma.o
 obj-$(CONFIG_DMA_SUN6I) += sun6i-dma.o
+obj-$(CONFIG_DMA_SUN8I) += sun8i-dma.o
 obj-$(CONFIG_DW_DMAC_CORE) += dw/
 obj-$(CONFIG_EP93XX_DMA) += ep93xx_dma.o
 obj-$(CONFIG_FSL_DMA) += fsldma.o
diff --git a/drivers/dma/sun6i-dma.c b/drivers/dma/sun6i-dma.c
index bcd496edc..af4ec5fb3 100644
--- a/drivers/dma/sun6i-dma.c
+++ b/drivers/dma/sun6i-dma.c
@@ -15,6 +15,7 @@
 #include <linux/delay.h>
 #include <linux/dmaengine.h>
 #include <linux/dmapool.h>
+#include <linux/dma/sunxi-dma.h>
 #include <linux/interrupt.h>
 #include <linux/module.h>
 #include <linux/of_dma.h>
@@ -46,6 +47,7 @@
  * sun8i specific registers
  */
 #define SUN8I_DMA_GATE		0x20
+#define SUN50I_DMA_GATE		0x28
 #define SUN8I_DMA_GATE_ENABLE	0x4
 
 /*
@@ -63,9 +65,9 @@
 
 #define DMA_CHAN_CUR_CFG	0x0c
 #define DMA_CHAN_CFG_SRC_DRQ(x)		((x) & 0x1f)
-#define DMA_CHAN_CFG_SRC_IO_MODE	BIT(5)
-#define DMA_CHAN_CFG_SRC_LINEAR_MODE	(0 << 5)
-#define DMA_CHAN_CFG_SRC_BURST(x)	(((x) & 0x3) << 7)
+#define DMA_CHAN_CFG_SRC_IO_MODE	BIT(8)
+#define DMA_CHAN_CFG_SRC_LINEAR_MODE	(0 << 8)
+#define DMA_CHAN_CFG_SRC_BURST(x)	(((x) & 0x3) << 6)
 #define DMA_CHAN_CFG_SRC_WIDTH(x)	(((x) & 0x3) << 9)
 
 #define DMA_CHAN_CFG_DST_DRQ(x)		(DMA_CHAN_CFG_SRC_DRQ(x) << 16)
@@ -88,7 +90,7 @@
  */
 #define LLI_LAST_ITEM	0xfffff800
 #define NORMAL_WAIT	8
-#define DRQ_SDRAM	1
+#define DRQ_SDRAM 	0 //DRQSRC_SRAM
 
 /*
  * Hardware channels / ports representation
@@ -112,6 +114,7 @@ struct sun6i_dma_config {
 	 * BSP kernel source code.
 	 */
 	bool gate_needed;
+	u32 gate_offset;
 };
 
 /*
@@ -134,7 +137,7 @@ struct sun6i_dma_lli {
 	 * or freeing it).
 	 */
 	struct sun6i_dma_lli	*v_lli_next;
-};
+} __attribute__((packed));
 
 
 struct sun6i_desc {
@@ -246,25 +249,37 @@ static inline void sun6i_dma_dump_chan_regs(struct sun6i_dma_dev *sdev,
 		readl(pchan->base + DMA_CHAN_CUR_PARA));
 }
 
+/*
+ * Fix sconfig's burst size according to sunxi_dmac. We need to convert them as:
+ * 1 -> 0, 4 -> 1, 8 -> 2, 16->3
+ *
+ * NOTE: burst size 2 is not supported by controller.
+ *
+ * This can be done by finding least significant bit set: n & (n - 1)
+ */
 static inline s8 convert_burst(u32 maxburst)
 {
-	switch (maxburst) {
-	case 1:
+	if (maxburst > 1)
+		return fls(maxburst) - 2;
+	else
 		return 0;
-	case 8:
-		return 2;
-	default:
-		return -EINVAL;
-	}
 }
 
+/*
+ * Fix sconfig's bus width according to at_dmac.
+ * 1 byte -> 0, 2 bytes -> 1, 4 bytes -> 2.
+ */
 static inline s8 convert_buswidth(enum dma_slave_buswidth addr_width)
 {
-	if ((addr_width < DMA_SLAVE_BUSWIDTH_1_BYTE) ||
-	    (addr_width > DMA_SLAVE_BUSWIDTH_4_BYTES))
-		return -EINVAL;
-
-	return addr_width >> 1;
+	switch (addr_width) {
+	case DMA_SLAVE_BUSWIDTH_2_BYTES:
+		return 1;
+	case DMA_SLAVE_BUSWIDTH_4_BYTES:
+		return 2;
+	default:
+		/* For 1 byte width or fallback */
+		return 0;
+	}
 }
 
 static size_t sun6i_get_chan_size(struct sun6i_pchan *pchan)
@@ -360,6 +375,7 @@ static int sun6i_dma_start_desc(struct sun6i_vchan *vchan)
 	struct sun6i_pchan *pchan = vchan->phy;
 	u32 irq_val, irq_reg, irq_offset;
 
+	printk("ROY: %s: %d\n", __func__, __LINE__);
 	if (!pchan)
 		return -EAGAIN;
 
@@ -534,17 +550,26 @@ static int set_config(struct sun6i_dma_dev *sdev,
 				DMA_SLAVE_BUSWIDTH_4_BYTES);
 		break;
 	default:
+		pr_err("dma-6i: unsupported dir: %d\n", direction);
 		return -EINVAL;
 	}
 
-	if (src_burst < 0)
+	if (src_burst < 0) {
+		pr_err("dma-6i: error src burst: %d\n", src_burst);
 		return src_burst;
-	if (src_width < 0)
+	}
+	if (src_width < 0) {
+		pr_err("dma-6i: err src width: %d\n", src_width);
 		return src_width;
-	if (dst_burst < 0)
+	}
+	if (dst_burst < 0) {
+		pr_err("dma-6i: err dst burst: %d\n", dst_burst);
 		return dst_burst;
-	if (dst_width < 0)
+	}
+	if (dst_width < 0) {
+		pr_err("dma-6i: err dst width: %d\n", dst_width);
 		return dst_width;
+	}
 
 	*p_cfg = DMA_CHAN_CFG_SRC_BURST(src_burst) |
 		DMA_CHAN_CFG_SRC_WIDTH(src_width) |
@@ -554,6 +579,30 @@ static int set_config(struct sun6i_dma_dev *sdev,
 	return 0;
 }
 
+static inline void sunxi_cfg_lli(struct sun6i_dma_lli *lli, dma_addr_t src,
+		dma_addr_t dst, u32 len, struct dma_slave_config *config)
+{
+	u32 src_width, dst_width;
+
+	if (!config)
+		return;
+
+	/* Get the data width */
+	src_width = convert_buswidth(config->src_addr_width);
+	dst_width = convert_buswidth(config->dst_addr_width);
+
+	lli->cfg = DMA_CHAN_CFG_SRC_BURST(config->src_maxburst)
+			| DMA_CHAN_CFG_SRC_WIDTH(src_width)
+			| DMA_CHAN_CFG_DST_BURST(config->dst_maxburst)
+			| DMA_CHAN_CFG_DST_WIDTH(dst_width);
+
+	lli->src = (u32)src;
+	lli->dst = (u32)dst;
+	lli->len = len;
+	lli->para = NORMAL_WAIT;
+
+}
+
 static struct dma_async_tx_descriptor *sun6i_dma_prep_dma_memcpy(
 		struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,
 		size_t len, unsigned long flags)
@@ -609,6 +658,63 @@ static struct dma_async_tx_descriptor *sun6i_dma_prep_dma_memcpy(
 	return NULL;
 }
 
+static struct dma_async_tx_descriptor *sun6i_prep_dma_sg(
+		struct dma_chan *chan,
+		struct scatterlist *dst_sg, unsigned int dst_nents,
+		struct scatterlist *src_sg, unsigned int src_nents,
+		unsigned long flags)
+{
+	struct sun6i_vchan *vchan = to_sun6i_vchan(chan);
+	struct sun6i_dma_dev *sdev = to_sun6i_dma_dev(chan->device);
+	struct dma_slave_config *sconfig = &vchan->cfg;
+	struct sun6i_desc *txd;
+	struct sun6i_dma_lli *l_item, *prev = NULL;
+	dma_addr_t p_lli;
+	// u32	phy;
+
+	if (dst_nents != src_nents)
+		return NULL;
+
+	if (!dst_nents || !src_nents)
+		return NULL;
+
+	if (dst_sg == NULL || src_sg == NULL)
+		return NULL;
+
+	txd = kzalloc(sizeof(*txd), GFP_NOWAIT);
+	if (!txd) {
+		dev_err(chan2dev(chan), "%s: Failed to alloc sunxi_desc!!\n", __func__);
+		return NULL;
+	}
+	vchan_tx_prep(&vchan->vc, &txd->vd, flags);
+
+	while ((src_sg != NULL) && (dst_sg != NULL)) {
+		l_item = dma_pool_alloc(sdev->pool, GFP_NOWAIT, &p_lli);
+		if (!l_item)
+			goto err_lli_free;
+
+		sunxi_cfg_lli(l_item, sg_dma_address(src_sg),
+				sg_dma_address(dst_sg), sg_dma_len(dst_sg), sconfig);
+		l_item->cfg |= DMA_CHAN_CFG_SRC_LINEAR_MODE
+			| DMA_CHAN_CFG_DST_LINEAR_MODE
+			| GET_DST_DRQ(sconfig->slave_id)
+			| GET_SRC_DRQ(sconfig->slave_id);
+
+		prev = sun6i_dma_lli_add(prev, l_item, p_lli, txd);
+		// prev = sunxi_lli_list(prev, l_item, p_lli, txd);
+		src_sg = sg_next(src_sg);
+		dst_sg = sg_next(dst_sg);
+	}
+
+	return &txd->vd.tx;
+
+err_lli_free:
+	for (prev = txd->v_lli; prev; prev = prev->v_lli_next)
+		dma_pool_free(sdev->pool, prev, virt_to_phys(prev));
+	kfree(txd);
+	return NULL;
+}
+
 static struct dma_async_tx_descriptor *sun6i_dma_prep_slave_sg(
 		struct dma_chan *chan, struct scatterlist *sgl,
 		unsigned int sg_len, enum dma_transfer_direction dir,
@@ -651,8 +757,8 @@ static struct dma_async_tx_descriptor *sun6i_dma_prep_slave_sg(
 			v_lli->cfg = lli_cfg |
 				DMA_CHAN_CFG_DST_IO_MODE |
 				DMA_CHAN_CFG_SRC_LINEAR_MODE |
-				DMA_CHAN_CFG_SRC_DRQ(DRQ_SDRAM) |
-				DMA_CHAN_CFG_DST_DRQ(vchan->port);
+				DMA_CHAN_CFG_SRC_DRQ(DRQ_SDRAM) | GET_DST_DRQ(sconfig->slave_id);
+				// DMA_CHAN_CFG_DST_DRQ(vchan->port);
 
 			dev_dbg(chan2dev(chan),
 				"%s; chan: %d, dest: %pad, src: %pad, len: %u. flags: 0x%08lx\n",
@@ -666,8 +772,8 @@ static struct dma_async_tx_descriptor *sun6i_dma_prep_slave_sg(
 			v_lli->cfg = lli_cfg |
 				DMA_CHAN_CFG_DST_LINEAR_MODE |
 				DMA_CHAN_CFG_SRC_IO_MODE |
-				DMA_CHAN_CFG_DST_DRQ(DRQ_SDRAM) |
-				DMA_CHAN_CFG_SRC_DRQ(vchan->port);
+				DMA_CHAN_CFG_DST_DRQ(DRQ_SDRAM) | GET_SRC_DRQ(sconfig->slave_id);
+				// DMA_CHAN_CFG_SRC_DRQ(vchan->port);
 
 			dev_dbg(chan2dev(chan),
 				"%s; chan: %d, dest: %pad, src: %pad, len: %u. flags: 0x%08lx\n",
@@ -736,16 +842,16 @@ static struct dma_async_tx_descriptor *sun6i_dma_prep_dma_cyclic(
 			v_lli->cfg = lli_cfg |
 				DMA_CHAN_CFG_DST_IO_MODE |
 				DMA_CHAN_CFG_SRC_LINEAR_MODE |
-				DMA_CHAN_CFG_SRC_DRQ(DRQ_SDRAM) |
-				DMA_CHAN_CFG_DST_DRQ(vchan->port);
+				DMA_CHAN_CFG_SRC_DRQ(DRQ_SDRAM) | GET_DST_DRQ(sconfig->slave_id);
+				// DMA_CHAN_CFG_DST_DRQ(vchan->port);
 		} else {
 			v_lli->src = sconfig->src_addr;
 			v_lli->dst = buf_addr + period_len * i;
 			v_lli->cfg = lli_cfg |
 				DMA_CHAN_CFG_DST_LINEAR_MODE |
 				DMA_CHAN_CFG_SRC_IO_MODE |
-				DMA_CHAN_CFG_DST_DRQ(DRQ_SDRAM) |
-				DMA_CHAN_CFG_SRC_DRQ(vchan->port);
+				DMA_CHAN_CFG_DST_DRQ(DRQ_SDRAM) | GET_SRC_DRQ(sconfig->slave_id);
+				// DMA_CHAN_CFG_SRC_DRQ(vchan->port);
 		}
 
 		prev = sun6i_dma_lli_add(prev, v_lli, p_lli, txd);
@@ -764,7 +870,7 @@ static struct dma_async_tx_descriptor *sun6i_dma_prep_dma_cyclic(
 	return NULL;
 }
 
-static int sun6i_dma_config(struct dma_chan *chan,
+static int sun6i_dma_set_config(struct dma_chan *chan,
 			    struct dma_slave_config *config)
 {
 	struct sun6i_vchan *vchan = to_sun6i_vchan(chan);
@@ -1021,6 +1127,7 @@ static struct sun6i_dma_config sun8i_a23_dma_cfg = {
 	.nr_max_requests = 24,
 	.nr_max_vchans   = 37,
 	.gate_needed	 = true,
+	.gate_offset	 = SUN8I_DMA_GATE,
 };
 
 static struct sun6i_dma_config sun8i_a83t_dma_cfg = {
@@ -1050,6 +1157,19 @@ static struct sun6i_dma_config sun8i_v3s_dma_cfg = {
 	.nr_max_requests = 23,
 	.nr_max_vchans   = 24,
 	.gate_needed	 = true,
+	.gate_offset	 = SUN8I_DMA_GATE,
+};
+
+/*
+ * The 50i have only 16 physical channels, a maximum DRQ port id of 45,
+ * and a total of 24 usable source and destination endpoints.
+ */
+static struct sun6i_dma_config sun50i_h6_dma_cfg = {
+	.nr_max_channels = 16,
+	.nr_max_requests = 45,
+	.nr_max_vchans   = 64,
+	.gate_needed	 = true,
+	.gate_offset	 = SUN50I_DMA_GATE,
 };
 
 static const struct of_device_id sun6i_dma_match[] = {
@@ -1058,6 +1178,7 @@ static const struct of_device_id sun6i_dma_match[] = {
 	{ .compatible = "allwinner,sun8i-a83t-dma", .data = &sun8i_a83t_dma_cfg },
 	{ .compatible = "allwinner,sun8i-h3-dma", .data = &sun8i_h3_dma_cfg },
 	{ .compatible = "allwinner,sun8i-v3s-dma", .data = &sun8i_v3s_dma_cfg },
+	{ .compatible = "allwinner,sun50i-dma", .data = &sun50i_h6_dma_cfg },
 	{ /* sentinel */ }
 };
 MODULE_DEVICE_TABLE(of, sun6i_dma_match);
@@ -1121,11 +1242,12 @@ static int sun6i_dma_probe(struct platform_device *pdev)
 	sdc->slave.device_free_chan_resources	= sun6i_dma_free_chan_resources;
 	sdc->slave.device_tx_status		= sun6i_dma_tx_status;
 	sdc->slave.device_issue_pending		= sun6i_dma_issue_pending;
+	// sdc->slave.device_prep_dma_sg 		= sun6i_prep_dma_sg;
 	sdc->slave.device_prep_slave_sg		= sun6i_dma_prep_slave_sg;
 	sdc->slave.device_prep_dma_memcpy	= sun6i_dma_prep_dma_memcpy;
 	sdc->slave.device_prep_dma_cyclic	= sun6i_dma_prep_dma_cyclic;
 	sdc->slave.copy_align			= DMAENGINE_ALIGN_4_BYTES;
-	sdc->slave.device_config		= sun6i_dma_config;
+	sdc->slave.device_config		= sun6i_dma_set_config;
 	sdc->slave.device_pause			= sun6i_dma_pause;
 	sdc->slave.device_resume		= sun6i_dma_resume;
 	sdc->slave.device_terminate_all		= sun6i_dma_terminate_all;
@@ -1200,7 +1322,7 @@ static int sun6i_dma_probe(struct platform_device *pdev)
 	}
 
 	if (sdc->cfg->gate_needed)
-		writel(SUN8I_DMA_GATE_ENABLE, sdc->base + SUN8I_DMA_GATE);
+		writel(SUN8I_DMA_GATE_ENABLE, sdc->base + sdc->cfg->gate_offset);
 
 	return 0;
 
diff --git a/drivers/dma/sun8i-dma.c b/drivers/dma/sun8i-dma.c
new file mode 100644
index 000000000..d7d3384f9
--- /dev/null
+++ b/drivers/dma/sun8i-dma.c
@@ -0,0 +1,1272 @@
+/*
+ * drivers/dma/sunxi-dma.c
+ *
+ * Copyright (C) 2013-2015 Allwinnertech Co., Ltd
+ *
+ * Author: Sugar <shuge@allwinnertech.com>
+ *
+ * Sunxi DMA controller driver
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#include <linux/bitops.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/dmapool.h>
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/debugfs.h>
+#include <linux/dma/sunxi-dma.h>
+#include <linux/sunxi-smc.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+
+#ifndef CONFIG_OF
+#include <mach/platform.h>
+#endif
+
+#include "dmaengine.h"
+#include "virt-dma.h"
+
+#define NR_MAX_CHAN	16			/* total of channels */
+
+#define HIGH_CHAN	8
+
+#define DMA_IRQ_EN(x)	(0x000 + ((x) << 2))	/* Interrupt enable register */
+#define DMA_IRQ_STAT(x)	(0x010 + ((x) << 2))	/* Inetrrupt status register */
+
+#define DMA_SECU	0x20			/* DMA security register */
+
+#define DMA_GATE	0x28			/* DMA gating rgister */
+
+#define DMA_STAT	0x30			/* DMA Status Register RO */
+#define DMA_ENABLE(x)	(0x100 + ((x) << 6))	/* Channels enable register */
+#define DMA_PAUSE(x)	(0x104 + ((x) << 6))	/* DMA Channels pause register */
+#define DMA_LLI_ADDR(x)	(0x108 + ((x) << 6))	/* Descriptor address register */
+#define DMA_CFG(x)	(0x10C + ((x) << 6))	/* Configuration register RO */
+#define DMA_CUR_SRC(x)	(0x110 + ((x) << 6))	/* Current source address RO */
+#define DMA_CUR_DST(x)	(0x114 + ((x) << 6))	/* Current destination address RO */
+#define DMA_CNT(x)	(0x118 + ((x) << 6))	/* Byte counter left register RO */
+#define DMA_PARA(x)	(0x11C + ((x) << 6))	/* Parameter register RO */
+
+#define LINK_END	0xFFFFF800		/* lastest link must be 0xfffff800 */
+
+/* DMA opertions mode */
+#define DMA_OP_MODE(x)	(0x128 + ((x) << 6))	/* DMA mode options register */
+#define SRC_HS_MASK	(0x1 << 2)		/* bit 2: Source handshark mode */
+#define DST_HS_MASK	(0x1 << 3)		/* bit 3: Destination handshark mode */
+
+#define SET_OP_MODE(d, x, val)	({	\
+		writel(val, d->base + DMA_OP_MODE(x));	\
+		})
+
+#define SHIFT_IRQ_MASK(val, ch) ({	\
+		(ch) >= HIGH_CHAN	\
+		? (val) << ((ch - HIGH_CHAN) << 2) \
+		: (val) << ((ch) << 2 );	\
+		})
+
+#define IRQ_HALF	0x01		/* Half package transfer interrupt pending */
+#define IRQ_PKG		0x02		/* One package complete interrupt pending */
+#define IRQ_QUEUE	0x04		/* All list complete transfer interrupt pending */
+
+/* The detail information of DMA configuration */
+#define SRC_WIDTH(x)	((x) << 9)
+#define SRC_BURST(x)	((x) << 6)
+
+#define SRC_IO_MODE	(0x01 << 8)
+#define SRC_LINEAR_MODE	(0x00 << 8)
+#define SRC_DRQ(x)	((x) << 0)
+
+#define DST_WIDTH(x)	((x) << 25)
+#define DST_BURST(x)	((x) << 22)
+
+#define DST_IO_MODE	(0x01 << 24)
+#define DST_LINEAR_MODE	(0x00 << 24)
+
+#define DST_DRQ(x)	((x) << 16)
+
+#define CHAN_START	1
+#define CHAN_STOP	0
+#define CHAN_PAUSE	1
+#define CHAN_RESUME	0
+
+#define NORMAL_WAIT	(8 << 0)
+
+/* lli: linked list ltem, the DMA block descriptor */
+struct sunxi_dma_lli {
+	u32	cfg;		/* DMA configuration */
+	u32	src;		/* Source address */
+	u32	dst;		/* Destination address */
+	u32	len;		/* Length of buffers */
+	u32	para;		/* Parameter register */
+	u32	p_lln;		/* Next lli physical address */
+	struct sunxi_dma_lli *v_lln;	/* Next lli virtual address (only for cpu) */
+#ifdef DEBUG
+	u32	this_phy;	/* Physical address of this lli */
+	#define set_this_phy(li, addr)	\
+		((li)->this_phy = (addr))
+#else
+	#define set_this_phy(li, addr)
+#endif
+}__attribute__((packed));
+
+struct sunxi_dmadev {
+	struct dma_device	dma_dev;
+	void __iomem		*base;
+	phys_addr_t             pbase;
+	struct clk		*ahb_clk;	/* AHB clock gate for DMA */
+
+	spinlock_t		lock;
+	struct tasklet_struct	task;
+	struct list_head	pending;	/* the pending channels list */
+	struct dma_pool		*lli_pool;	/* Pool of lli */
+};
+
+struct sunxi_desc {
+	struct virt_dma_desc	vd;
+	u32			lli_phys;	/* physical start for llis */
+	struct sunxi_dma_lli	*lli_virt;	/* virtual start for lli */
+};
+
+struct sunxi_chan {
+	struct virt_dma_chan	vc;
+
+	struct list_head	node;		/* queue it to pending list */
+	struct dma_slave_config	cfg;
+	bool			cyclic;
+
+	struct sunxi_desc	*desc;
+	u32			irq_type;
+};
+
+static u64 sunxi_dma_mask = DMA_BIT_MASK(32);
+
+static inline struct sunxi_dmadev *to_sunxi_dmadev(struct dma_device *d)
+{
+	return container_of(d, struct sunxi_dmadev, dma_dev);
+}
+
+static inline struct sunxi_chan *to_sunxi_chan(struct dma_chan *chan)
+{
+	return container_of(chan, struct sunxi_chan, vc.chan);
+}
+
+static inline struct sunxi_desc *to_sunxi_desc(struct dma_async_tx_descriptor *tx)
+{
+	return container_of(tx, struct sunxi_desc, vd.tx);
+}
+
+static struct device *chan2dev(struct dma_chan *chan)
+{
+	return &chan->dev->device;
+}
+static struct device *chan2parent(struct dma_chan *chan)
+{
+	return chan->dev->device.parent;
+}
+
+/*
+ * Fix sconfig's burst size according to sunxi_dmac. We need to convert them as:
+ * 1 -> 0, 4 -> 1, 8 -> 2, 16->3
+ *
+ * NOTE: burst size 2 is not supported by controller.
+ *
+ * This can be done by finding least significant bit set: n & (n - 1)
+ */
+static inline void convert_burst(u32 *maxburst)
+{
+	if (*maxburst > 1)
+		*maxburst = fls(*maxburst) - 2;
+	else
+		*maxburst = 0;
+}
+
+/*
+ * Fix sconfig's bus width according to at_dmac.
+ * 1 byte -> 0, 2 bytes -> 1, 4 bytes -> 2.
+ */
+static inline u8 convert_buswidth(enum dma_slave_buswidth addr_width)
+{
+	switch (addr_width) {
+	case DMA_SLAVE_BUSWIDTH_2_BYTES:
+		return 1;
+	case DMA_SLAVE_BUSWIDTH_4_BYTES:
+		return 2;
+	default:
+		/* For 1 byte width or fallback */
+		return 0;
+	}
+}
+
+static size_t sunxi_get_desc_size(struct sunxi_desc *txd)
+{
+	struct sunxi_dma_lli *lli;
+	size_t size = 0;
+
+	for (lli = txd->lli_virt; lli != NULL; lli = lli->v_lln)
+		size += lli->len;
+
+	return size;
+}
+
+/*
+ * sunxi_get_chan_size - get the bytes left of one channel.
+ * @ch: the channel
+ */
+static size_t sunxi_get_chan_size(struct sunxi_chan *ch)
+{
+	struct sunxi_dma_lli *lli;
+	struct sunxi_desc *txd;
+	struct sunxi_dmadev *sdev;
+	size_t size = 0;
+	u32 pos;
+	bool count = false;
+
+	txd = ch->desc;
+
+	if (!txd)
+		return 0;
+
+	sdev = to_sunxi_dmadev(ch->vc.chan.device);
+	pos = readl(sdev->base + DMA_LLI_ADDR(ch->vc.chan.chan_id));
+	size = readl(sdev->base + DMA_CNT(ch->vc.chan.chan_id));
+
+	/* It is the last package, and just read count register */
+	if (pos == LINK_END)
+		return size;
+
+	for (lli = txd->lli_virt; lli != NULL; lli = lli->v_lln) {
+		/* Ok, found next lli that is ready be transported */
+		if (lli->p_lln == pos) {
+			count = true;
+			continue;
+		}
+
+		if (count)
+			size += lli->len;
+	}
+
+	return size;
+}
+
+/*
+ * sunxi_free_desc - free the struct sunxi_desc.
+ * @vd: the virt-desc for this chan
+ */
+static void sunxi_free_desc(struct virt_dma_desc *vd)
+{
+	struct sunxi_desc *txd = to_sunxi_desc(&vd->tx);
+	struct sunxi_dmadev *sdev = to_sunxi_dmadev(vd->tx.chan->device);
+	struct sunxi_dma_lli *li_adr, *next_virt;
+	u32 phy, next_phy;
+
+	if (unlikely(!txd))
+		return;
+
+	phy = txd->lli_phys;
+	li_adr = txd->lli_virt;
+
+	while(li_adr) {
+		next_virt = li_adr->v_lln;
+		next_phy = li_adr->p_lln;
+		dma_pool_free(sdev->lli_pool, li_adr, phy);
+		li_adr = next_virt;
+		phy = next_phy;
+	}
+
+	txd->vd.tx.callback = NULL;
+	txd->vd.tx.callback_param = NULL;
+	kfree(txd);
+	txd = NULL;
+}
+
+static inline void sunxi_dump_com_regs(struct sunxi_chan *ch)
+{
+	struct sunxi_dmadev *sdev;
+
+	sdev = to_sunxi_dmadev(ch->vc.chan.device);
+
+	pr_debug("Common register:\n"
+			"\tmask0(%04x): 0x%08x\n"
+			"\tmask1(%04x): 0x%08x\n"
+			"\tpend0(%04x): 0x%08x\n"
+			"\tpend1(%04x): 0x%08x\n"
+#ifdef DMA_SECU
+			"\tsecur(%04x): 0x%08x\n"
+#endif
+#ifdef DMA_GATE
+			"\t_gate(%04x): 0x%08x\n"
+#endif
+			"\tstats(%04x): 0x%08x\n",
+			DMA_IRQ_EN(0),  readl(sdev->base + DMA_IRQ_EN(0)),
+			DMA_IRQ_EN(1),  readl(sdev->base + DMA_IRQ_EN(1)),
+			DMA_IRQ_STAT(0),readl(sdev->base + DMA_IRQ_STAT(0)),
+			DMA_IRQ_STAT(1),readl(sdev->base + DMA_IRQ_STAT(1)),
+#ifdef DMA_SECU
+			DMA_SECU, readl(sdev->base + DMA_SECU),
+#endif
+#ifdef DMA_GATE
+			DMA_GATE, readl(sdev->base + DMA_GATE),
+#endif
+			DMA_STAT, readl(sdev->base + DMA_STAT));
+}
+
+static inline void sunxi_dump_chan_regs(struct sunxi_chan *ch)
+{
+	struct sunxi_dmadev *sdev = to_sunxi_dmadev(ch->vc.chan.device);
+	u32 chan_num = ch->vc.chan.chan_id;
+
+	pr_debug("Chan %d reg:\n"
+			"\t___en(%04x): \t0x%08x\n"
+			"\tpause(%04x): \t0x%08x\n"
+			"\tstart(%04x): \t0x%08x\n"
+			"\t__cfg(%04x): \t0x%08x\n"
+			"\t__src(%04x): \t0x%08x\n"
+			"\t__dst(%04x): \t0x%08x\n"
+			"\tcount(%04x): \t0x%08x\n"
+			"\t_para(%04x): \t0x%08x\n\n",
+			chan_num,
+			DMA_ENABLE(chan_num),
+			readl(sdev->base + DMA_ENABLE(chan_num)),
+			DMA_PAUSE(chan_num),
+			readl(sdev->base + DMA_PAUSE(chan_num)),
+			DMA_LLI_ADDR(chan_num),
+			readl(sdev->base + DMA_LLI_ADDR(chan_num)),
+			DMA_CFG(chan_num),
+			readl(sdev->base + DMA_CFG(chan_num)),
+			DMA_CUR_SRC(chan_num),
+			readl(sdev->base + DMA_CUR_SRC(chan_num)),
+			DMA_CUR_DST(chan_num),
+			readl(sdev->base + DMA_CUR_DST(chan_num)),
+			DMA_CNT(chan_num),
+			readl(sdev->base + DMA_CNT(chan_num)),
+			DMA_PARA(chan_num),
+			readl(sdev->base + DMA_PARA(chan_num)));
+}
+
+
+/*
+ * sunxi_dma_resume - resume channel, which is pause sate.
+ * @ch: the channel to resume
+ */
+static int sunxi_dma_resume(struct dma_chan *ch)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(ch);
+	struct sunxi_dmadev *sdev = to_sunxi_dmadev(schan->vc.chan.device);
+	u32 chan_num = schan->vc.chan.chan_id;
+
+	printk("ROY: %s: %d\n", __func__, __LINE__);
+	writel(CHAN_RESUME, sdev->base + DMA_PAUSE(chan_num));
+
+	return 0;
+}
+
+static int sunxi_dma_pause(struct dma_chan *ch)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(ch);
+	struct sunxi_dmadev *sdev = to_sunxi_dmadev(schan->vc.chan.device);
+	u32 chan_num = schan->vc.chan.chan_id;
+
+	printk("ROY: %s: %d\n", __func__, __LINE__);
+	writel(CHAN_PAUSE, sdev->base + DMA_PAUSE(chan_num));
+
+	return 0;
+}
+
+/*
+ * sunxi_terminate_all - stop all descriptors that waiting transfer on chan.
+ * @ch: the channel to stop
+ */
+static int sunxi_terminate_all(struct dma_chan *ch)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(ch);
+	struct sunxi_dmadev *sdev = to_sunxi_dmadev(schan->vc.chan.device);
+	struct virt_dma_desc *vd = NULL;
+	struct virt_dma_chan *vc = NULL;
+	u32 chan_num = schan->vc.chan.chan_id;
+	unsigned long flags;
+	LIST_HEAD(head);
+
+	printk("ROY: %s: %d\n", __func__, __LINE__);
+
+	spin_lock_irqsave(&schan->vc.lock, flags);
+
+	spin_lock(&sdev->lock);
+	list_del_init(&schan->node);
+	spin_unlock(&sdev->lock);
+
+	/* We should entry PAUSE state first to avoid missing data
+	 * count which transferring on bus.
+	 */
+	writel(CHAN_PAUSE, sdev->base + DMA_PAUSE(chan_num));
+	writel(CHAN_STOP, sdev->base + DMA_ENABLE(chan_num));
+	writel(CHAN_RESUME, sdev->base + DMA_PAUSE(chan_num));
+
+	/* At cyclic mode, desc is not be managed by virt-dma,
+	 * we need to add it to desc_completed
+	 */
+	if (schan->cyclic) {
+		schan->cyclic = false;
+		if (schan->desc) {
+			vd = &(schan->desc->vd);
+			vc = &(schan->vc);
+			list_add_tail(&vd->node, &vc->desc_completed);
+		}
+	}
+	schan->desc = NULL;
+
+	vchan_get_all_descriptors(&schan->vc, &head);
+	spin_unlock_irqrestore(&schan->vc.lock, flags);
+	vchan_dma_desc_free_list(&schan->vc, &head);
+
+	return 0;
+}
+
+/*
+ * sunxi_start_desc - begin to transport the descriptor
+ * @ch: the channel of descriptor
+ */
+static void sunxi_start_desc(struct sunxi_chan *ch)
+{
+	struct virt_dma_desc *vd = vchan_next_desc(&ch->vc);
+	struct sunxi_dmadev *sdev = to_sunxi_dmadev(ch->vc.chan.device);
+	struct sunxi_desc *txd = NULL;
+	u32 chan_num = ch->vc.chan.chan_id;
+	u32 irq_val;
+	u32 high;
+
+	printk("ROY: %s: %d\n", __func__, __LINE__);
+	if (!vd){
+		while(readl(sdev->base + DMA_STAT) & (1 << chan_num))
+			cpu_relax();
+		writel(CHAN_STOP, sdev->base + DMA_ENABLE(chan_num));
+		return;
+	}
+
+	/* Delete this desc from the desc_issued list */
+	list_del(&vd->node);
+
+	txd = to_sunxi_desc(&vd->tx);
+	ch->desc = txd;
+
+	if (ch->cyclic)
+		ch->irq_type = IRQ_PKG;
+	else
+		ch->irq_type = IRQ_QUEUE;
+
+	high = (chan_num >= HIGH_CHAN) ? 1 : 0;
+
+	irq_val = readl(sdev->base + DMA_IRQ_EN(high));
+	irq_val |= SHIFT_IRQ_MASK(ch->irq_type, chan_num);
+	writel(irq_val, sdev->base + DMA_IRQ_EN(high));
+
+	/* Set the DMA opertions mode */
+	SET_OP_MODE(sdev, chan_num, SRC_HS_MASK | DST_HS_MASK);
+
+	/* write the first lli address to register, and start to transfer */
+	writel(txd->lli_phys, sdev->base + DMA_LLI_ADDR(chan_num));
+	writel(CHAN_START, sdev->base + DMA_ENABLE(chan_num));
+
+	sunxi_dump_com_regs(ch);
+	sunxi_dump_chan_regs(ch);
+}
+
+/*
+ * sunxi_alloc_lli - Allocate a sunxi_lli
+ * @sdev: the sunxi_dmadev
+ * @phy_addr: return the physical address
+ */
+void *sunxi_alloc_lli(struct sunxi_dmadev *sdev, u32 *phy_addr)
+{
+	struct sunxi_dma_lli *l_item;
+	dma_addr_t phy;
+
+	WARN_TAINT(!sdev->lli_pool, TAINT_WARN, "The dma pool is empty!!\n");
+	if (unlikely(!sdev->lli_pool))
+		return NULL;
+
+	l_item = dma_pool_alloc(sdev->lli_pool, GFP_ATOMIC, &phy);
+	*phy_addr = (u32)phy;
+	set_this_phy(l_item, *phy_addr);
+
+	return l_item;
+}
+
+/*
+ * sunxi_dump_lli - dump the information for one lli
+ * @shcan: the channel
+ * @lli: a lli to dump
+ */
+static inline void sunxi_dump_lli(struct sunxi_chan *schan, struct sunxi_dma_lli *lli)
+{
+#ifdef	DEBUG
+	dev_dbg(chan2dev(&schan->vc.chan),
+			"\n\tdesc:   p - 0x%08x v - 0x%08x \n"
+			"\t\tc - 0x%08x s - 0x%08x d - 0x%08x\n"
+			"\t\tl - 0x%08x p - 0x%08x n - 0x%08x\n",
+			lli->this_phy, (u32)lli,
+			lli->cfg, lli->src, lli->dst,
+			lli->len, lli->para, lli->p_lln);
+#endif
+}
+
+static void *sunxi_lli_list(struct sunxi_dma_lli *prev, struct sunxi_dma_lli *next,
+		u32 next_phy, struct sunxi_desc *txd)
+{
+	if ((!prev && !txd) || !next)
+		return NULL;
+
+	if (!prev){
+		txd->lli_phys = next_phy;
+		txd->lli_virt = next;
+	} else {
+		prev->p_lln = next_phy;
+		prev->v_lln = next;
+	}
+
+	next->p_lln = LINK_END;
+	next->v_lln = NULL;
+
+	return next;
+}
+
+static inline void sunxi_cfg_lli(struct sunxi_dma_lli *lli, dma_addr_t src,
+		dma_addr_t dst, u32 len, struct dma_slave_config *config)
+{
+	u32 src_width, dst_width;
+
+	if (!config)
+		return;
+
+	/* Get the data width */
+	src_width = convert_buswidth(config->src_addr_width);
+	dst_width = convert_buswidth(config->dst_addr_width);
+
+	lli->cfg = SRC_BURST(config->src_maxburst)
+			| SRC_WIDTH(src_width)
+			| DST_BURST(config->dst_maxburst)
+			| DST_WIDTH(dst_width);
+
+	lli->src = (u32)src;
+	lli->dst = (u32)dst;
+	lli->len = len;
+	lli->para = NORMAL_WAIT;
+
+}
+
+
+/*
+ * sunxi_dma_tasklet - ensure that the desc's lli be putted into hardware.
+ * @data: sunxi_dmadev
+ */
+static void sunxi_dma_tasklet(unsigned long data)
+{
+	struct sunxi_dmadev *sdev = (struct sunxi_dmadev *)data;
+	LIST_HEAD(head);
+
+	spin_lock_irq(&sdev->lock);
+	list_splice_tail_init(&sdev->pending, &head);
+	spin_unlock_irq(&sdev->lock);
+
+	while (!list_empty(&head)) {
+		struct sunxi_chan *c = list_first_entry(&head,
+			struct sunxi_chan, node);
+
+		spin_lock_irq(&c->vc.lock);
+		list_del_init(&c->node);
+		sunxi_start_desc(c);
+		spin_unlock_irq(&c->vc.lock);
+	}
+}
+
+/*
+ * sunxi_dma_interrupt - interrupt handle.
+ * @irq: irq number
+ * @dev_id: sunxi_dmadev
+ */
+static irqreturn_t sunxi_dma_interrupt(int irq, void *dev_id)
+{
+	struct sunxi_dmadev *sdev = (struct sunxi_dmadev *)dev_id;
+	struct sunxi_chan *ch;
+	struct sunxi_desc *desc;
+	unsigned long flags;
+	u32 status_lo = 0, status_hi = 0;
+
+	/* Get the status of irq */
+	status_lo = readl(sdev->base + DMA_IRQ_STAT(0));
+#if NR_MAX_CHAN > HIGH_CHAN
+	status_hi = readl(sdev->base + DMA_IRQ_STAT(1));
+#endif
+
+	dev_dbg(sdev->dma_dev.dev, "[sunxi_dma]: DMA irq status_lo: 0x%08x, "
+			"status_hi: 0x%08x\n", status_lo, status_hi);
+
+	/* Clear the bit of irq status */
+	writel(status_lo, sdev->base + DMA_IRQ_STAT(0));
+#if NR_MAX_CHAN > HIGH_CHAN
+	writel(status_hi, sdev->base + DMA_IRQ_STAT(1));
+#endif
+
+	list_for_each_entry(ch, &sdev->dma_dev.channels, vc.chan.device_node) {
+		u32 chan_num = ch->vc.chan.chan_id;
+		u32 status;
+
+		status = (chan_num >= HIGH_CHAN)
+			? (status_hi >> ((chan_num - HIGH_CHAN) <<2))
+			: (status_lo >> (chan_num << 2));
+
+		spin_lock_irqsave(&ch->vc.lock, flags);
+		if (!(ch->irq_type & status))
+			goto unlock;
+
+		if (!ch->desc)
+			goto unlock;
+
+		desc = ch->desc;
+		if (ch->cyclic) {
+			struct virt_dma_desc *vd;
+			dma_async_tx_callback cb = NULL;
+			void *cb_data = NULL;
+
+			vd = &desc->vd;
+			if (vd) {
+				cb = vd->tx.callback;
+				cb_data = vd->tx.callback_param;
+			}
+			spin_unlock_irqrestore(&ch->vc.lock, flags);
+			if (cb)
+				cb(cb_data);
+			spin_lock_irqsave(&ch->vc.lock, flags);
+		} else {
+			ch->desc = NULL;
+			vchan_cookie_complete(&desc->vd);
+			sunxi_start_desc(ch);
+		}
+unlock:
+		spin_unlock_irqrestore(&ch->vc.lock, flags);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static struct dma_async_tx_descriptor *sunxi_prep_dma_memcpy(
+		struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,
+		size_t len, unsigned long flags)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	struct sunxi_desc *txd;
+	struct sunxi_dma_lli *l_item;
+	struct sunxi_dmadev *sdev = to_sunxi_dmadev(chan->device);
+	struct dma_slave_config *sconfig = &schan->cfg;
+	u32	phy;
+
+	dev_dbg(chan2dev(chan), "%s; chan: %d, dest: 0x%08lx, "
+			"src: 0x%08lx, len: 0x%08zx. flags: 0x%08lx\n",
+			__func__, schan->vc.chan.chan_id, (unsigned long)dest,
+			(unsigned long)src, len, flags);
+
+	if (unlikely(!len)) {
+		dev_dbg(chan2dev(chan), "%s: memcpy length is zero!!\n", __func__);
+		return NULL;
+	}
+
+	txd = kzalloc(sizeof(*txd), GFP_NOWAIT);
+	if (!txd) {
+		dev_err(chan2dev(chan), "%s: Failed to alloc sunxi_desc!!\n", __func__);
+		return NULL;
+	}
+	vchan_tx_prep(&schan->vc, &txd->vd, flags);
+
+	l_item = sunxi_alloc_lli(sdev, &phy);
+	if (!l_item) {
+		sunxi_free_desc(&txd->vd);
+		dev_err(sdev->dma_dev.dev, "Failed to alloc lli memory!!!\n");
+		return NULL;
+	}
+
+	sunxi_cfg_lli(l_item, src, dest, len, sconfig);
+	l_item->cfg |= SRC_DRQ(DRQSRC_SDRAM)
+			| DST_DRQ(DRQDST_SDRAM)
+			| DST_LINEAR_MODE
+			| SRC_LINEAR_MODE;
+
+	sunxi_lli_list(NULL, l_item, phy, txd);
+
+	sunxi_dump_lli(schan, l_item);
+
+	return &txd->vd.tx;
+}
+
+static struct dma_async_tx_descriptor *sunxi_prep_dma_sg(
+		struct dma_chan *chan,
+		struct scatterlist *dst_sg, unsigned int dst_nents,
+		struct scatterlist *src_sg, unsigned int src_nents,
+		unsigned long flags)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	struct sunxi_dmadev *sdev = to_sunxi_dmadev(chan->device);
+	struct dma_slave_config *sconfig = &schan->cfg;
+	struct sunxi_desc *txd;
+	struct sunxi_dma_lli *l_item, *prev = NULL;
+	u32	phy;
+
+	if (dst_nents != src_nents)
+		return NULL;
+
+	if (!dst_nents || !src_nents)
+		return NULL;
+
+	if (dst_sg == NULL || src_sg == NULL)
+		return NULL;
+
+	txd = kzalloc(sizeof(*txd), GFP_NOWAIT);
+	if (!txd) {
+		dev_err(chan2dev(chan), "%s: Failed to alloc sunxi_desc!!\n", __func__);
+		return NULL;
+	}
+	vchan_tx_prep(&schan->vc, &txd->vd, flags);
+
+	while ((src_sg != NULL) && (dst_sg != NULL)) {
+		l_item = sunxi_alloc_lli(sdev, &phy);
+		if (!l_item) {
+			sunxi_free_desc(&txd->vd);
+			return NULL;
+		}
+
+		sunxi_cfg_lli(l_item, sg_dma_address(src_sg),
+				sg_dma_address(dst_sg), sg_dma_len(dst_sg), sconfig);
+		l_item->cfg |= SRC_LINEAR_MODE
+			| DST_LINEAR_MODE
+			| GET_DST_DRQ(sconfig->slave_id)
+			| GET_SRC_DRQ(sconfig->slave_id);
+
+		prev = sunxi_lli_list(prev, l_item, phy, txd);
+		src_sg = sg_next(src_sg);
+		dst_sg = sg_next(dst_sg);
+	}
+
+#ifdef DEBUG
+	pr_debug("[sunxi_dma]: First: 0x%08x\n", txd->lli_phys);
+	for(prev = txd->lli_virt; prev != NULL; prev = prev->v_lln)
+		sunxi_dump_lli(schan, prev);
+#endif
+
+	return &txd->vd.tx;
+}
+
+static struct dma_async_tx_descriptor *sunxi_prep_slave_sg(
+		struct dma_chan *chan, struct scatterlist *sgl,
+		unsigned int sg_len, enum dma_transfer_direction dir,
+		unsigned long flags, void *context)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	struct sunxi_desc *txd;
+	struct sunxi_dma_lli *l_item, *prev = NULL;
+	struct sunxi_dmadev *sdev = to_sunxi_dmadev(chan->device);
+	struct dma_slave_config *sconfig = &schan->cfg;
+
+	struct scatterlist *sg;
+	u32	phy;
+	unsigned int i;
+
+	if (unlikely(!sg_len)) {
+		dev_dbg(chan2dev(chan), "%s: sg length is zero!!\n", __func__);
+		return NULL;
+	}
+
+	txd = kzalloc(sizeof(*txd), GFP_NOWAIT);
+	if (!txd) {
+		dev_err(chan2dev(chan), "%s: Failed to alloc sunxi_desc!!\n", __func__);
+		return NULL;
+	}
+	vchan_tx_prep(&schan->vc, &txd->vd, flags);
+
+	for_each_sg(sgl, sg, sg_len, i) {
+		l_item = sunxi_alloc_lli(sdev, &phy);
+		if (!l_item) {
+			sunxi_free_desc(&txd->vd);
+			return NULL;
+		}
+
+		if (dir == DMA_MEM_TO_DEV) {
+			sunxi_cfg_lli(l_item, sg_dma_address(sg),
+					sconfig->dst_addr, sg_dma_len(sg), sconfig);
+			l_item->cfg |= DST_IO_MODE
+					| SRC_LINEAR_MODE
+					| SRC_DRQ(DRQSRC_SDRAM)
+					| GET_DST_DRQ(sconfig->slave_id);
+
+		} else if (dir == DMA_DEV_TO_MEM) {
+			sunxi_cfg_lli(l_item, sconfig->src_addr,
+					sg_dma_address(sg), sg_dma_len(sg), sconfig);
+			l_item->cfg |= DST_LINEAR_MODE
+					| SRC_IO_MODE
+					| DST_DRQ(DRQDST_SDRAM)
+					| GET_SRC_DRQ(sconfig->slave_id);
+		}
+
+		prev = sunxi_lli_list(prev, l_item, phy, txd);
+	}
+
+#ifdef DEBUG
+	pr_debug("[sunxi_dma]: First: 0x%08x\n", txd->lli_phys);
+	for(prev = txd->lli_virt; prev != NULL; prev = prev->v_lln)
+		sunxi_dump_lli(schan, prev);
+#endif
+
+	return &txd->vd.tx;
+}
+
+/**
+ * sunxi_prep_dma_cyclic - prepare the cyclic DMA transfer
+ * @chan: the DMA channel to prepare
+ * @buf_addr: physical DMA address where the buffer starts
+ * @buf_len: total number of bytes for the entire buffer
+ * @period_len: number of bytes for each period
+ * @dir: transfer direction, to or from device
+ *
+ * Must be called before trying to start the transfer. Returns a valid struct
+ * sunxi_cyclic_desc if successful or an ERR_PTR(-errno) if not successful.
+ */
+struct dma_async_tx_descriptor *sunxi_prep_dma_cyclic( struct dma_chan *chan,
+		dma_addr_t buf_addr, size_t buf_len, size_t period_len,
+		enum dma_transfer_direction dir, unsigned long flags)
+{
+	struct sunxi_desc *txd;
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	struct sunxi_dmadev *sdev = to_sunxi_dmadev(chan->device);
+	struct sunxi_dma_lli *l_item, *prev = NULL;
+	struct dma_slave_config *sconfig = &schan->cfg;
+
+	u32 phy;
+	unsigned int periods = buf_len / period_len;
+	unsigned int i;
+
+	/*
+	 * Not allow duplicate prep dma on cyclic channel.
+	 */
+	if (schan->desc && schan->cyclic)
+		return NULL;
+
+	txd = kzalloc(sizeof(*txd), GFP_NOWAIT);
+	if (!txd) {
+		dev_err(chan2dev(chan), "%s: Failed to alloc sunxi_desc!!\n", __func__);
+		return NULL;
+	}
+	vchan_tx_prep(&schan->vc, &txd->vd, flags);
+
+	for (i = 0; i < periods; i++){
+		l_item = sunxi_alloc_lli(sdev, &phy);
+		if (!l_item) {
+			sunxi_free_desc(&txd->vd);
+			return NULL;
+		}
+
+
+		if (dir == DMA_MEM_TO_DEV) {
+			sunxi_cfg_lli(l_item, (buf_addr + period_len * i),
+					sconfig->dst_addr, period_len, sconfig);
+			l_item->cfg |= GET_DST_DRQ(sconfig->slave_id)
+					| SRC_LINEAR_MODE
+					| DST_IO_MODE
+					| SRC_DRQ(DRQSRC_SDRAM);
+		} else if (dir == DMA_DEV_TO_MEM) {
+			sunxi_cfg_lli(l_item, sconfig->src_addr,
+					(buf_addr + period_len * i), period_len, sconfig);
+			l_item->cfg |= GET_SRC_DRQ(sconfig->slave_id)
+					| DST_LINEAR_MODE
+					| SRC_IO_MODE
+					| DST_DRQ(DRQDST_SDRAM);
+		} else if (dir == DMA_DEV_TO_DEV) {
+			sunxi_cfg_lli(l_item, sconfig->src_addr,
+					sconfig->dst_addr, period_len, sconfig);
+			l_item->cfg |= GET_SRC_DRQ(sconfig->slave_id)
+					| DST_IO_MODE
+					| SRC_IO_MODE
+					| GET_DST_DRQ(sconfig->slave_id);
+		}
+
+		prev = sunxi_lli_list(prev, l_item, phy, txd);
+
+	}
+
+	/* Make a cyclic list */
+	prev->p_lln = txd->lli_phys;
+	schan->cyclic = true;
+
+#ifdef DEBUG
+	pr_debug("[sunxi_dma]: First: 0x%08x\n", txd->lli_phys);
+	for(prev = txd->lli_virt; prev != NULL; prev = prev->v_lln)
+		sunxi_dump_lli(schan, prev);
+#endif
+
+	return &txd->vd.tx;
+}
+
+static int sunxi_set_runtime_config(struct dma_chan *chan,
+		struct dma_slave_config *sconfig)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+
+	memcpy(&schan->cfg, sconfig, sizeof(struct dma_slave_config));
+
+	convert_burst(&schan->cfg.src_maxburst);
+	convert_burst(&schan->cfg.dst_maxburst);
+
+	return 0;
+}
+
+static enum dma_status sunxi_tx_status(struct dma_chan *chan,
+	      dma_cookie_t cookie, struct dma_tx_state *txstate)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	struct virt_dma_desc *vd;
+	enum dma_status ret;
+	unsigned long flags;
+	size_t bytes = 0;
+
+	ret = dma_cookie_status(chan, cookie, txstate);
+	if (ret == DMA_COMPLETE || !txstate) {
+		return ret;
+	}
+
+	spin_lock_irqsave(&schan->vc.lock, flags);
+	vd = vchan_find_desc(&schan->vc, cookie);
+	if (vd) {
+		bytes = sunxi_get_desc_size(to_sunxi_desc(&vd->tx));
+	} else if (schan->desc && schan->desc->vd.tx.cookie == cookie) {
+		bytes = sunxi_get_chan_size(to_sunxi_chan(chan));
+	}
+
+	/*
+	 * This cookie not complete yet
+	 * Get number of bytes left in the active transactions and queue
+	 */
+	dma_set_residue(txstate, bytes);
+	spin_unlock_irqrestore(&schan->vc.lock, flags);
+
+	return ret;
+}
+
+/*
+ * sunxi_issue_pending - try to finish work
+ * @chan: target DMA channel
+ *
+ * It will call vchan_issue_pending(), which can move the desc_submitted
+ * list to desc_issued list. And we will move the chan to pending list of
+ * sunxi_dmadev.
+ */
+static void sunxi_issue_pending(struct dma_chan *chan)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	struct sunxi_dmadev *sdev = to_sunxi_dmadev(chan->device);
+	unsigned long flags;
+
+	spin_lock_irqsave(&schan->vc.lock, flags);
+	if (vchan_issue_pending(&schan->vc) && !schan->desc) {
+		if (schan->cyclic){
+			sunxi_start_desc(schan);
+			goto out;
+		}
+
+		spin_lock(&sdev->lock);
+		if (list_empty(&schan->node))
+			list_add_tail(&schan->node, &sdev->pending);
+		spin_unlock(&sdev->lock);
+		tasklet_schedule(&sdev->task);
+	}
+out:
+	spin_unlock_irqrestore(&schan->vc.lock, flags);
+}
+
+static int sunxi_alloc_chan_resources(struct dma_chan *chan)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	dev_dbg(chan2parent(chan), "%s: Now alloc chan resources!\n", __func__);
+
+	schan->cyclic = false;
+
+	return 0;
+}
+
+/*
+ * sunxi_free_chan_resources - free the resources of channel
+ * @chan: the channel to free
+ */
+static void sunxi_free_chan_resources(struct dma_chan *chan)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+
+	vchan_free_chan_resources(&schan->vc);
+
+	dev_dbg(chan2parent(chan), "%s: Now free chan resources!!\n", __func__);
+}
+
+/*
+ * sunxi_chan_free - free the channle on dmadevice
+ * @sdev: the dmadevice of sunxi
+ */
+static inline void sunxi_chan_free(struct sunxi_dmadev *sdev)
+{
+	struct sunxi_chan *ch;
+
+	tasklet_kill(&sdev->task);
+	while(!list_empty(&sdev->dma_dev.channels)) {
+		ch = list_first_entry(&sdev->dma_dev.channels,
+				struct sunxi_chan, vc.chan.device_node);
+		list_del(&ch->vc.chan.device_node);
+		tasklet_kill(&ch->vc.task);
+		kfree(ch);
+	}
+
+}
+
+static void sunxi_dma_hw_init(struct sunxi_dmadev *dev)
+{
+	struct sunxi_dmadev *sunxi_dev = dev;
+
+	clk_prepare_enable(sunxi_dev->ahb_clk);
+#if defined(CONFIG_SUNXI_SMC)
+	sunxi_smc_writel((1 << NR_MAX_CHAN) - 1, sunxi_dev->pbase + DMA_SECU);
+#endif
+
+#if 0
+	writel(0x05, sunxi_dev->base + DMA_GATE);
+#endif
+}
+
+static int sunxi_probe(struct platform_device *pdev)
+{
+	struct sunxi_dmadev *sunxi_dev;
+	struct sunxi_chan *schan;
+	struct resource *res;
+	int irq;
+	int ret, i;
+
+	pdev->dev.dma_mask = &sunxi_dma_mask;
+	pdev->dev.coherent_dma_mask = DMA_BIT_MASK(32);
+
+	sunxi_dev = kzalloc(sizeof(struct sunxi_dmadev), GFP_KERNEL);
+	if (!sunxi_dev)
+		return -ENOMEM;
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		ret = -EINVAL;
+		goto io_err;
+	}
+
+	sunxi_dev->pbase = res->start;
+	sunxi_dev->base = ioremap(res->start, resource_size(res));
+	if (!sunxi_dev->base) {
+		dev_err(&pdev->dev, "Remap I/O memory failed!\n");
+		ret = -ENOMEM;
+		goto io_err;
+	}
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0) {
+		ret = irq;
+		goto irq_err;
+	}
+
+	ret = request_irq(irq, sunxi_dma_interrupt, IRQF_SHARED,
+			dev_name(&pdev->dev), sunxi_dev);
+	if (ret) {
+		dev_err(&pdev->dev, "NO IRQ found!!!\n");
+		goto irq_err;
+	}
+
+	sunxi_dev->ahb_clk = clk_get(&pdev->dev, "dma");
+	if (!sunxi_dev->ahb_clk) {
+		dev_err(&pdev->dev, "NO clock to dma!!!\n");
+		ret = -EINVAL;
+		goto clk_err;
+	}
+	sunxi_dev->lli_pool = dma_pool_create(dev_name(&pdev->dev), &pdev->dev,
+			sizeof(struct sunxi_dma_lli), 4/* word alignment */, 0);
+	if (!sunxi_dev->lli_pool) {
+		ret = -ENOMEM;
+		goto pool_err;
+	}
+
+	platform_set_drvdata(pdev, sunxi_dev);
+	INIT_LIST_HEAD(&sunxi_dev->pending);
+	spin_lock_init(&sunxi_dev->lock);
+
+	/* Initialize dmaengine */
+	dma_cap_set(DMA_MEMCPY, sunxi_dev->dma_dev.cap_mask);
+	dma_cap_set(DMA_SLAVE, sunxi_dev->dma_dev.cap_mask);
+	dma_cap_set(DMA_CYCLIC, sunxi_dev->dma_dev.cap_mask);
+	dma_cap_set(DMA_MEMSET_SG, sunxi_dev->dma_dev.cap_mask);
+
+	INIT_LIST_HEAD(&sunxi_dev->dma_dev.channels);
+	sunxi_dev->dma_dev.device_alloc_chan_resources	= sunxi_alloc_chan_resources;
+	sunxi_dev->dma_dev.device_free_chan_resources	= sunxi_free_chan_resources;
+	sunxi_dev->dma_dev.device_tx_status		= sunxi_tx_status;
+	sunxi_dev->dma_dev.device_pause			= sunxi_dma_pause;
+	sunxi_dev->dma_dev.device_resume		= sunxi_dma_resume;
+	sunxi_dev->dma_dev.device_terminate_all = sunxi_terminate_all;
+	sunxi_dev->dma_dev.device_issue_pending		= sunxi_issue_pending;
+	// sunxi_dev->dma_dev.device_prep_dma_sg		= sunxi_prep_dma_sg;
+	sunxi_dev->dma_dev.device_prep_slave_sg		= sunxi_prep_slave_sg;
+	sunxi_dev->dma_dev.device_prep_dma_cyclic	= sunxi_prep_dma_cyclic;
+	sunxi_dev->dma_dev.device_prep_dma_memcpy	= sunxi_prep_dma_memcpy;
+	sunxi_dev->dma_dev.device_config		= sunxi_set_runtime_config;
+
+	sunxi_dev->dma_dev.dev = &pdev->dev;
+
+	tasklet_init(&sunxi_dev->task, sunxi_dma_tasklet, (unsigned long)sunxi_dev);
+
+	for (i = 0; i < NR_MAX_CHAN; i++){
+		schan = kzalloc(sizeof(*schan), GFP_KERNEL);
+		if (!schan){
+			dev_err(&pdev->dev, "%s: no memory for channel\n", __func__);
+			ret = -ENOMEM;
+			goto chan_err;
+		}
+		INIT_LIST_HEAD(&schan->node);
+		sunxi_dev->dma_dev.chancnt++;
+		schan->vc.desc_free = sunxi_free_desc;
+		vchan_init(&schan->vc, &sunxi_dev->dma_dev);
+	}
+
+	/* Register the sunxi-dma to dmaengine */
+	ret = dma_async_device_register(&sunxi_dev->dma_dev);
+	if (ret) {
+		dev_warn(&pdev->dev, "Failed to register DMA engine device: %d\n", ret);
+		goto chan_err;
+	}
+
+	/* All is ok, and open the clock */
+	sunxi_dma_hw_init(sunxi_dev);
+
+	return 0;
+
+chan_err:
+	sunxi_chan_free(sunxi_dev);
+	platform_set_drvdata(pdev, NULL);
+	dma_pool_destroy(sunxi_dev->lli_pool);
+pool_err:
+	clk_put(sunxi_dev->ahb_clk);
+clk_err:
+	free_irq(irq, sunxi_dev);
+irq_err:
+	iounmap(sunxi_dev->base);
+io_err:
+	kfree(sunxi_dev);
+	return ret;
+}
+
+static int sunxi_remove(struct platform_device *pdev)
+{
+	struct sunxi_dmadev *sunxi_dev = platform_get_drvdata(pdev);
+
+	dma_async_device_unregister(&sunxi_dev->dma_dev);
+
+	sunxi_chan_free(sunxi_dev);
+
+	free_irq(platform_get_irq(pdev, 0), sunxi_dev);
+	dma_pool_destroy(sunxi_dev->lli_pool);
+	clk_disable_unprepare(sunxi_dev->ahb_clk);
+	clk_put(sunxi_dev->ahb_clk);
+	iounmap(sunxi_dev->base);
+	kfree(sunxi_dev);
+
+	return 0;
+}
+
+static void sunxi_shutdown(struct platform_device *pdev)
+{
+	struct sunxi_dmadev *sdev = platform_get_drvdata(pdev);
+
+	clk_disable_unprepare(sdev->ahb_clk);
+}
+
+static int sunxi_suspend_noirq(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct sunxi_dmadev *sunxi_dev = platform_get_drvdata(pdev);
+
+	clk_disable_unprepare(sunxi_dev->ahb_clk);
+	return 0;
+}
+
+static int sunxi_resume_noirq(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct sunxi_dmadev *sunxi_dev = platform_get_drvdata(pdev);
+
+	sunxi_dma_hw_init(sunxi_dev);
+	return 0;
+}
+
+static const struct dev_pm_ops sunxi_dev_pm_ops = {
+	.suspend_noirq = sunxi_suspend_noirq,
+	.resume_noirq = sunxi_resume_noirq,
+	.freeze_noirq = sunxi_suspend_noirq,
+	.thaw_noirq = sunxi_resume_noirq,
+	.restore_noirq = sunxi_resume_noirq,
+	.poweroff_noirq = sunxi_suspend_noirq,
+};
+
+static const struct of_device_id sunxi_dma_match[] = {
+        { .compatible = "allwinner,sun50i-dma", },
+		{ .compatible = "allwinner,sun8i-dma", },
+        {},
+};
+
+static struct platform_driver sunxi_dma_driver = {
+	.probe		= sunxi_probe,
+	.remove		= sunxi_remove,
+	.shutdown	= sunxi_shutdown,
+	.driver = {
+		.name	= "sunxi_dmac",
+		.pm	= &sunxi_dev_pm_ops,
+		.of_match_table = sunxi_dma_match,
+	},
+};
+
+bool sunxi_dma_filter_fn(struct dma_chan *chan, void *param)
+{
+	bool ret = false;
+	if (chan->device->dev->driver == &sunxi_dma_driver.driver){
+		const char *p = param;
+		ret = !strcmp("sunxi_dmac", p);
+		pr_debug("[sunxi_rdma]: sunxi_dma_filter_fn: %s\n", p);
+	}
+	return ret;
+}
+EXPORT_SYMBOL_GPL(sunxi_dma_filter_fn);
+
+static int __init sunxi_dma_init(void)
+{
+	int ret;
+	ret = platform_driver_register(&sunxi_dma_driver);
+
+	return ret;
+}
+subsys_initcall(sunxi_dma_init);
+
+static void __exit sunxi_dma_exit(void)
+{
+	platform_driver_unregister(&sunxi_dma_driver);
+}
+module_exit(sunxi_dma_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Sunxi DMA Controller driver");
+MODULE_AUTHOR("Shuge");
+MODULE_ALIAS("platform:sunxi_dmac");
diff --git a/drivers/dma/sunxi-rdma.c b/drivers/dma/sunxi-rdma.c
new file mode 100644
index 000000000..74dbc6386
--- /dev/null
+++ b/drivers/dma/sunxi-rdma.c
@@ -0,0 +1,973 @@
+/*
+ * drivers/dma/sunxi-dma.c
+ *
+ * Copyright (C) 2013-2015 Allwinnertech Co., Ltd
+ *
+ * Author: Sugar <shuge@allwinnertech.com>
+ *
+ * Sunxi DMA controller driver
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#include <linux/bitops.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/dmapool.h>
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/debugfs.h>
+#include <linux/dma/sunxi-dma.h>
+#include <linux/kthread.h>
+#include <mach/platform.h>
+#include <mach/sunxi-smc.h>
+
+#include "dmaengine.h"
+#include "virt-dma.h"
+
+#define NR_MAX_CHAN	4			/* total of channels */
+
+#define RDMA_IRQ_ID	SUNXI_IRQ_R_DMA		/* SUN9I: 67 */
+#define RDMA_PHYS_BASE	SUNXI_R_DMA_PBASE	/* SUN9I: 0x08008000 */
+
+#define RDMA_IRQ_EN	(0x00)			/* Interrupt enable register */
+#define RDMA_IRQ_PEN	(0x04)			/* Inetrrupt status register */
+#define RDMA_GATE	(0x08)			/* Auto gating register */
+#define RDMA_CTRL(x)	(0x100 + ((x) << 5))	/* Control register */
+#define RDMA_SRC(x)	(0x104 + ((x) << 5))
+#define RDMA_DST(x)	(0x108 + ((x) << 5))
+#define RDMA_CNT(x)	(0x10c + ((x) << 5))
+
+
+#define SHIFT_IRQ_MASK(val, ch) ({	\
+		(val) << ((ch) << 1 );	\
+		})
+
+#define IRQ_HALF	0x01		/* Half package transfer interrupt pending */
+#define IRQ_PKG		0x02		/* One package complete interrupt pending */
+
+/* The detail information of RDMA control register */
+#define SRC_DRQ(x)	((x) << 0)
+#define SRC_IO_MODE	(0x01 << 5)
+#define SRC_LINEAR_MODE	(0x00 << 5)
+#define SRC_BURST(x)	((x) << 7)
+#define SRC_WIDTH(x)	((x) << 8)
+
+#define REMAIN_MODE	(1 << 15)
+
+#define DST_DRQ(x)	((x) << 16)
+#define DST_IO_MODE	(0x01 << 21)
+#define DST_LINEAR_MODE	(0x00 << 21)
+#define DST_BURST(x)	((x) << 23)
+#define DST_WIDTH(x)	((x) << 24)
+
+#define CONTI_MODE	(1 << 29)
+#define	BUSY_BIT	(1 << 30)
+#define CHAN_START	(1 << 31)
+
+#define WAIT_STATE	(7 << 26)
+
+#define RDMA_MAX_BYTES	((18 << 1) -1)UL
+
+/* lli: linked list ltem, the DMA block descriptor */
+struct sunxi_rdma_lli {
+	u32		cfg;		/* The configuration fo R_DMA */
+	dma_addr_t	src;		/* Source address */
+	dma_addr_t	dst;		/* Destination address */
+	u32		len;		/* Length of buffers */
+	struct sunxi_rdma_lli *v_lln;	/* Next lli virtual address (only for cpu) */
+}__attribute__((packed));
+
+struct sunxi_rdmadev {
+	struct dma_device	dma_dev;
+	void __iomem		*base;
+	struct clk		*apbs_clk;	/* AHB clock gate for DMA */
+
+	spinlock_t		lock;
+	struct tasklet_struct	task;
+	struct list_head	pending;	/* the pending channels list */
+};
+
+struct sunxi_desc {
+	struct virt_dma_desc	vd;
+	struct sunxi_rdma_lli	*head;		/* Mark the head of list */
+	struct sunxi_rdma_lli	*cur_lli;	/* virtual start for lli */
+};
+
+struct sunxi_chan {
+	struct virt_dma_chan	vc;
+
+	struct list_head	node;		/* queue it to pending list */
+	struct dma_slave_config	cfg;
+	bool	cyclic;
+
+	struct sunxi_desc	*desc;
+	u32	irq_type;
+};
+
+static inline struct sunxi_rdmadev *to_sunxi_dmadev(struct dma_device *d)
+{
+	return container_of(d, struct sunxi_rdmadev, dma_dev);
+}
+
+static inline struct sunxi_chan *to_sunxi_chan(struct dma_chan *chan)
+{
+	return container_of(chan, struct sunxi_chan, vc.chan);
+}
+
+static inline struct sunxi_desc *to_sunxi_desc(struct dma_async_tx_descriptor *tx)
+{
+	return container_of(tx, struct sunxi_desc, vd.tx);
+}
+
+static inline struct device *chan2dev(struct dma_chan *chan)
+{
+	return &chan->dev->device;
+}
+static inline struct device *chan2parent(struct dma_chan *chan)
+{
+	return chan->dev->device.parent;
+}
+
+/*
+ * Fix sconfig's burst size according to sunxi_dmac. We need to convert them as:
+ * 1 -> 0, 4 -> 1, 8 -> 2, 16->3
+ *
+ * NOTE: burst size 2 is not supported by controller.
+ *
+ * This can be done by finding least significant bit set: n & (n - 1)
+ */
+static inline void convert_burst(u32 *maxburst)
+{
+#if 0
+	if (*maxburst > 1 && *maxburst < 8)
+		*maxburst = fls(*maxburst) - 2;
+	else
+#endif
+		*maxburst = 0;
+}
+
+/*
+ * Fix sconfig's bus width according to at_dmac.
+ * 1 byte -> 0, 2 bytes -> 1, 4 bytes -> 2.
+ */
+static inline u8 convert_buswidth(enum dma_slave_buswidth addr_width)
+{
+	switch (addr_width) {
+	case DMA_SLAVE_BUSWIDTH_2_BYTES:
+		return 1;
+	case DMA_SLAVE_BUSWIDTH_4_BYTES:
+		return 2;
+	default:
+		/* For 1 byte width or fallback */
+		return 0;
+	}
+}
+
+static size_t sunxi_get_desc_size(struct sunxi_desc *txd)
+{
+	struct sunxi_rdma_lli *lli;
+	size_t size = 0;
+
+	for (lli = txd->cur_lli; lli != NULL; lli = lli->v_lln)
+		size += lli->len;
+
+	return size;
+}
+
+/*
+ * sunxi_get_chan_size - get the bytes left of one channel.
+ * @ch: the channel
+ */
+static size_t sunxi_get_chan_size(struct sunxi_chan *ch)
+{
+	struct sunxi_rdma_lli *lli;
+	struct sunxi_desc *txd;
+	struct sunxi_rdmadev *sdev;
+	size_t size = 0;
+	u32 chan_num = ch->vc.chan.chan_id;
+	u32 ctrl_reg;
+
+	txd = ch->desc;
+
+	if (!txd)
+		return 0;
+
+	sdev = to_sunxi_dmadev(ch->vc.chan.device);
+
+	ctrl_reg = sunxi_smc_readl(sdev->base + RDMA_CTRL(chan_num));
+	sunxi_smc_writel(REMAIN_MODE | ctrl_reg, sdev->base + RDMA_CTRL(chan_num));
+	size = sunxi_smc_readl(sdev->base + RDMA_CNT(chan_num));
+	ctrl_reg = sunxi_smc_readl(sdev->base + RDMA_CTRL(chan_num));
+	sunxi_smc_writel(ctrl_reg & (~REMAIN_MODE), sdev->base + RDMA_CTRL(chan_num));
+
+	for (lli = txd->cur_lli->v_lln; lli != NULL; lli = lli->v_lln)
+		size += lli->len;
+
+	return size;
+}
+
+/*
+ * sunxi_free_desc - free the struct sunxi_desc.
+ * @vd: the virt-desc for this chan
+ */
+static void sunxi_free_desc(struct virt_dma_desc *vd)
+{
+	struct sunxi_desc *txd = to_sunxi_desc(&vd->tx);
+	struct sunxi_rdma_lli *li_adr, *next_virt;
+
+	if (unlikely(!txd))
+		return;
+
+	li_adr = txd->cur_lli;
+	while(li_adr != NULL) {
+		next_virt = li_adr->v_lln;
+		kfree(li_adr);
+		li_adr = next_virt;
+	}
+
+	kfree(txd);
+}
+
+static inline void sunxi_dump_com_regs(struct sunxi_chan *ch)
+{
+	struct sunxi_rdmadev *sdev;
+
+	sdev = to_sunxi_dmadev(ch->vc.chan.device);
+
+	pr_debug("Common Register: \n"
+			"\t\tIRQ_EN (0x00): 0x%08x\n"
+			"\t\tIRQ_PEN(0x04): 0x%08x\n"
+			"\t\tGATE   (0x08): 0x%08x\n",
+			sunxi_smc_readl(sdev->base + RDMA_IRQ_EN),
+			sunxi_smc_readl(sdev->base + RDMA_IRQ_PEN),
+			sunxi_smc_readl(sdev->base + RDMA_GATE));
+}
+
+static inline void sunxi_dump_chan_regs(struct sunxi_chan *ch)
+{
+	struct sunxi_rdmadev *sdev = to_sunxi_dmadev(ch->vc.chan.device);
+	u32 chan_num = ch->vc.chan.chan_id;
+
+	pr_debug("CHAN: %d, Register:\n"
+			"\t\tRDMA_SRC (0x%04x): 0x%08x\n"
+			"\t\tRDMA_DST (0x%04x): 0x%08x\n"
+			"\t\tRDMA_CTRL(0x%04x): 0x%08x\n"
+			"\t\tRDMA_CNT (0x%04x): 0x%08x\n",
+			chan_num,
+			RDMA_SRC(chan_num),
+			sunxi_smc_readl(sdev->base + RDMA_SRC(chan_num)),
+			RDMA_DST(chan_num),
+			sunxi_smc_readl(sdev->base + RDMA_DST(chan_num)),
+			RDMA_CTRL(chan_num),
+			sunxi_smc_readl(sdev->base + RDMA_CTRL(chan_num)),
+			RDMA_CNT(chan_num),
+			sunxi_smc_readl(sdev->base + RDMA_CNT(chan_num)));
+}
+
+
+/*
+ * sunxi_terminate_all - stop all descriptors that waiting transfer on chan.
+ * @ch: the channel to stop
+ */
+static int sunxi_terminate_all(struct sunxi_chan *ch)
+{
+	struct sunxi_rdmadev *sdev = to_sunxi_dmadev(ch->vc.chan.device);
+	u32 chan_num = ch->vc.chan.chan_id;
+	unsigned long flags;
+	LIST_HEAD(head);
+
+	spin_lock_irqsave(&ch->vc.lock, flags);
+
+	spin_lock(&sdev->lock);
+	list_del_init(&ch->node);
+	spin_unlock(&sdev->lock);
+
+	if (ch->desc)
+		ch->desc = NULL;
+
+	ch->cyclic = false;
+
+	sunxi_smc_writel(~CHAN_START, sdev->base + RDMA_CTRL(chan_num));
+
+	vchan_get_all_descriptors(&ch->vc, &head);
+	spin_unlock_irqrestore(&ch->vc.lock, flags);
+	vchan_dma_desc_free_list(&ch->vc, &head);
+
+	return 0;
+}
+
+/*
+ * sunxi_start_desc - begin to transport the descriptor
+ * @ch: the channel of descriptor
+ */
+static void sunxi_start_desc(struct sunxi_chan *ch)
+{
+	struct virt_dma_desc *vd = vchan_next_desc(&ch->vc);
+	struct sunxi_desc *txd = to_sunxi_desc(&vd->tx);
+	struct sunxi_rdmadev *sdev = to_sunxi_dmadev(ch->vc.chan.device);
+	u32 chan_num = ch->vc.chan.chan_id;
+	u32 value;
+
+	if (!vd) {
+		ch->desc = NULL;
+		return;
+	}
+
+	list_del(&vd->node);
+
+	ch->desc = txd;
+
+	while(sunxi_smc_readl(sdev->base + RDMA_CTRL(chan_num)) & BUSY_BIT)
+			cpu_relax();
+
+	ch->irq_type = IRQ_PKG;
+
+	if (ch->cyclic){
+		ch->irq_type |= IRQ_HALF;
+		sunxi_smc_writel(1 << 16, sdev->base + RDMA_GATE);
+	}
+
+	value = sunxi_smc_readl(sdev->base + RDMA_IRQ_EN);
+	value |= SHIFT_IRQ_MASK(ch->irq_type, chan_num);
+	sunxi_smc_writel(value, sdev->base + RDMA_IRQ_EN);
+
+	/* write the first lli address to register, and start to transfer */
+	sunxi_smc_writel(txd->cur_lli->src, sdev->base + RDMA_SRC(chan_num));
+	sunxi_smc_writel(txd->cur_lli->dst, sdev->base + RDMA_DST(chan_num));
+	sunxi_smc_writel(txd->cur_lli->len, sdev->base + RDMA_CNT(chan_num));
+	sunxi_smc_writel(txd->cur_lli->cfg | CHAN_START, sdev->base + RDMA_CTRL(chan_num));
+
+	sunxi_dump_com_regs(ch);
+	sunxi_dump_chan_regs(ch);
+}
+
+/*
+ * sunxi_dump_lli - dump the information for one lli
+ * @shcan: the channel
+ * @lli: a lli to dump
+ */
+static inline void sunxi_dump_lli(struct sunxi_chan *schan, struct sunxi_rdma_lli *head)
+{
+	struct sunxi_rdma_lli *lli;
+	pr_debug("[sunxi_rdma]: List of rdma_lli:\n");
+	for (lli = head; lli != NULL; lli = lli->v_lln)
+		pr_debug("\t\tcfg: 0x%08x, src: 0x%08x, dst: 0x%08x, "
+				"len: 0x%08x, next: 0x%08x\n",
+				(unsigned int)lli->cfg, (unsigned int)lli->src,
+				(unsigned int)lli->dst, (unsigned int)lli->len,
+				(unsigned int)lli->v_lln);
+}
+
+static void *sunxi_lli_list(struct sunxi_rdma_lli *prev,
+		struct sunxi_rdma_lli *next,
+		struct sunxi_desc *txd)
+{
+	if ((!prev && !txd) || !next)
+		return NULL;
+
+	if (!prev)
+		txd->head = txd->cur_lli = next;
+	else
+		prev->v_lln = next;
+
+	next->v_lln = NULL;
+
+	return next;
+}
+
+static inline void sunxi_cfg_lli(struct sunxi_rdma_lli *lli, dma_addr_t src,
+		dma_addr_t dst, u32 len, struct dma_slave_config *config)
+{
+	u32 src_width, dst_width;
+
+	if (!config)
+		return;
+
+	/* Get the data width */
+	src_width = convert_buswidth(config->src_addr_width);
+	dst_width = convert_buswidth(config->dst_addr_width);
+
+	lli->cfg = SRC_BURST(config->src_maxburst)
+			| SRC_WIDTH(src_width)
+			| DST_BURST(config->dst_maxburst)
+			| DST_WIDTH(dst_width)
+			| WAIT_STATE;
+
+	lli->src = src;
+	lli->dst = dst;
+	lli->len = len;
+}
+
+
+/*
+ * sunxi_dma_tasklet - ensure that the desc's lli be putted into hardware.
+ * @data: sunxi_rdmadev
+ */
+static void sunxi_rdma_tasklet(unsigned long data)
+{
+	struct sunxi_rdmadev *sdev = (struct sunxi_rdmadev *)data;
+	LIST_HEAD(head);
+
+	spin_lock_irq(&sdev->lock);
+	list_splice_tail_init(&sdev->pending, &head);
+	spin_unlock_irq(&sdev->lock);
+
+	while (!list_empty(&head)) {
+		struct sunxi_chan *c = list_first_entry(&head,
+			struct sunxi_chan, node);
+
+		spin_lock_irq(&c->vc.lock);
+		list_del_init(&c->node);
+		sunxi_start_desc(c);
+		spin_unlock_irq(&c->vc.lock);
+	}
+}
+
+/*
+ * sunxi_lli_load - set the next peroid for cyclic mode
+ * @chan: the channels to transfer
+ */
+static struct sunxi_rdma_lli *sunxi_lli_load(struct sunxi_chan *chan)
+{
+	struct sunxi_rdma_lli *l_item;
+	struct sunxi_rdmadev *sdev = to_sunxi_dmadev(chan->vc.chan.device);
+	u32 chan_num = chan->vc.chan.chan_id;
+
+	if (!chan->desc || !chan->desc->cur_lli)
+		return NULL;
+
+	l_item = chan->desc->cur_lli->v_lln;
+
+	if (!l_item) {
+		if (chan->cyclic) {
+			l_item = chan->desc->head;
+		} else
+			return NULL;
+	}
+
+	sunxi_smc_writel(l_item->src, sdev->base + RDMA_SRC(chan_num));
+	sunxi_smc_writel(l_item->dst, sdev->base + RDMA_DST(chan_num));
+
+	return l_item;
+}
+
+/*
+ * sunxi_rdma_interrupt - interrupt handle.
+ * @irq: irq number
+ * @dev_id: sunxi_rdmadev
+ */
+static irqreturn_t sunxi_rdma_interrupt(int irq, void *dev_id)
+{
+	struct sunxi_rdmadev *sdev = (struct sunxi_rdmadev *)dev_id;
+	struct sunxi_chan *ch;
+	struct sunxi_desc *desc;
+	unsigned long flags;
+	u32 status = 0;
+
+	/* Get the status of irq */
+	status = sunxi_smc_readl(sdev->base + RDMA_IRQ_PEN);
+	/* Clear the bit of irq status */
+	sunxi_smc_writel(status, sdev->base + RDMA_IRQ_PEN);
+
+	dev_dbg(sdev->dma_dev.dev, "[sunxi_rdma]: DMA irq status: 0x%08x\n", status);
+
+	list_for_each_entry(ch, &sdev->dma_dev.channels, vc.chan.device_node) {
+		u32 chan_num = ch->vc.chan.chan_id;
+
+		if (!(SHIFT_IRQ_MASK(ch->irq_type, chan_num) & status))
+			continue;
+
+		if (!ch->desc)
+			continue;
+
+		spin_lock_irqsave(&ch->vc.lock, flags);
+		desc = ch->desc;
+
+		if (ch->cyclic) {
+			if (status & IRQ_HALF) {
+				sunxi_lli_load(ch);
+			}
+			if (status & IRQ_PKG) {
+				if (desc->cur_lli && !desc->cur_lli->v_lln)
+					desc->cur_lli = desc->head;
+				else
+					desc->cur_lli = desc->cur_lli->v_lln;
+			}
+			vchan_cyclic_callback(&desc->vd);
+		} else {
+			vchan_cookie_complete(&desc->vd);
+			sunxi_start_desc(ch);
+		}
+		spin_unlock_irqrestore(&ch->vc.lock, flags);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static struct dma_async_tx_descriptor *sunxi_prep_rdma_memcpy(
+		struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,
+		size_t len, unsigned long flags)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	struct sunxi_desc *txd;
+	struct sunxi_rdma_lli *l_item;
+	struct sunxi_rdmadev *sdev = to_sunxi_dmadev(chan->device);
+	struct dma_slave_config *sconfig = &schan->cfg;
+
+	dev_dbg(chan2dev(chan), "%s; chan: %d, dest: 0x%08x, "
+			"src: 0x%08x, len: 0x%08x. flags: 0x%08lx\n",
+			__func__, schan->vc.chan.chan_id, dest, src, len, flags);
+
+	if (unlikely(!len)) {
+		dev_dbg(chan2dev(chan), "%s: memcpy length is zero!!\n", __func__);
+		return NULL;
+	}
+
+	txd = kzalloc(sizeof(*txd), GFP_NOWAIT);
+	if (!txd) {
+		dev_err(chan2dev(chan), "%s: Failed to alloc sunxi_desc!!\n", __func__);
+		return NULL;
+	}
+	vchan_tx_prep(&schan->vc, &txd->vd, flags);
+
+	l_item = kzalloc(sizeof(struct sunxi_rdma_lli), GFP_NOWAIT);
+	if (!l_item) {
+		sunxi_free_desc(&txd->vd);
+		dev_err(sdev->dma_dev.dev, "Failed to alloc lli memory!!!\n");
+		return NULL;
+	}
+
+	sunxi_cfg_lli(l_item, src, dest, len, sconfig);
+	l_item->cfg |= SRC_DRQ(DRQSRC_SDRAM)
+			| DST_DRQ(DRQDST_SDRAM)
+			| DST_LINEAR_MODE
+			| SRC_LINEAR_MODE;
+
+	sunxi_lli_list(NULL, l_item, txd);
+	sunxi_dump_lli(schan, txd->head);
+
+	return &txd->vd.tx;
+}
+
+
+/**
+ * sunxi_prep_rdma_cyclic - prepare the cyclic DMA transfer
+ * @chan: the DMA channel to prepare
+ * @buf_addr: physical DMA address where the buffer starts
+ * @buf_len: total number of bytes for the entire buffer
+ * @period_len: number of bytes for each period
+ * @dir: transfer direction, to or from device
+ *
+ * Must be called before trying to start the transfer. Returns a valid struct
+ * sunxi_cyclic_desc if successful or an ERR_PTR(-errno) if not successful.
+ */
+struct dma_async_tx_descriptor *sunxi_prep_rdma_cyclic( struct dma_chan *chan,
+		dma_addr_t buf_addr, size_t buf_len, size_t period_len,
+		enum dma_transfer_direction dir, unsigned long flags, void *context)
+{
+	struct sunxi_desc *txd;
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	struct sunxi_rdma_lli *l_item, *prev = NULL;
+	struct dma_slave_config *sconfig = &schan->cfg;
+
+	unsigned int periods = buf_len / period_len;
+	unsigned int i;
+
+	txd = kzalloc(sizeof(*txd), GFP_NOWAIT);
+	if (!txd) {
+		dev_err(chan2dev(chan), "%s: Failed to alloc sunxi_desc!!\n", __func__);
+		return NULL;
+	}
+	vchan_tx_prep(&schan->vc, &txd->vd, flags);
+
+	for (i = 0; i < periods; i++){
+		l_item = kzalloc(sizeof(struct sunxi_rdma_lli), GFP_NOWAIT);
+		if (!l_item) {
+			sunxi_free_desc(&txd->vd);
+			return NULL;
+		}
+
+		if (dir == DMA_MEM_TO_DEV) {
+			sunxi_cfg_lli(l_item, (buf_addr + period_len * i),
+					sconfig->dst_addr, period_len, sconfig);
+			l_item->cfg |= GET_DST_DRQ(sconfig->slave_id)
+					| SRC_LINEAR_MODE
+					| DST_IO_MODE
+					| SRC_DRQ(DRQSRC_SDRAM);
+		} else if (dir == DMA_DEV_TO_MEM) {
+			sunxi_cfg_lli(l_item, sconfig->src_addr,
+					(buf_addr + period_len * i), period_len, sconfig);
+			l_item->cfg |= GET_SRC_DRQ(sconfig->slave_id)
+					| DST_LINEAR_MODE
+					| SRC_IO_MODE
+					| DST_DRQ(DRQDST_SDRAM);
+		}
+
+		l_item->cfg |= CONTI_MODE;
+		prev = sunxi_lli_list(prev, l_item, txd);
+	}
+
+	/* Make a cyclic list */
+	schan->cyclic = true;
+
+	sunxi_dump_lli(schan, txd->head);
+
+	return &txd->vd.tx;
+}
+
+static int sunxi_set_runtime_config(struct dma_chan *chan,
+		struct dma_slave_config *sconfig)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+
+	memcpy(&schan->cfg, sconfig, sizeof(struct dma_slave_config));
+
+	convert_burst(&schan->cfg.src_maxburst);
+	convert_burst(&schan->cfg.dst_maxburst);
+
+	return 0;
+}
+
+static int sunxi_rdma_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,
+		       unsigned long arg)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	int ret = 0;
+
+	switch(cmd) {
+	case DMA_RESUME:
+		break;
+	case DMA_PAUSE:
+		break;
+	case DMA_TERMINATE_ALL:
+		ret = sunxi_terminate_all(schan);
+		break;
+	case DMA_SLAVE_CONFIG:
+		ret = sunxi_set_runtime_config(chan, (struct dma_slave_config *)arg);
+		break;
+	default:
+		ret = -ENXIO;
+		break;
+	}
+	return ret;
+}
+
+static enum dma_status sunxi_tx_status(struct dma_chan *chan,
+	      dma_cookie_t cookie, struct dma_tx_state *txstate)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	struct virt_dma_desc *vd;
+	enum dma_status ret;
+	unsigned long flags;
+	size_t bytes = 0;
+
+	ret = dma_cookie_status(chan, cookie, txstate);
+	if (ret == DMA_SUCCESS || !txstate) {
+		return ret;
+	}
+
+	spin_lock_irqsave(&schan->vc.lock, flags);
+	vd = vchan_find_desc(&schan->vc, cookie);
+	if (vd) {
+		bytes = sunxi_get_desc_size(to_sunxi_desc(&vd->tx));
+	} else if (schan->desc && schan->desc->vd.tx.cookie == cookie) {
+		bytes = sunxi_get_chan_size(to_sunxi_chan(chan));
+	}
+
+	/*
+	 * This cookie not complete yet
+	 * Get number of bytes left in the active transactions and queue
+	 */
+	dma_set_residue(txstate, bytes);
+	spin_unlock_irqrestore(&schan->vc.lock, flags);
+
+	return ret;
+}
+
+/*
+ * sunxi_issue_pending - try to finish work
+ * @chan: target DMA channel
+ *
+ * It will call vchan_issue_pending(), which can move the desc_submitted
+ * list to desc_issued list. And we will move the chan to pending list of
+ * sunxi_rdmadev.
+ */
+static void sunxi_issue_pending(struct dma_chan *chan)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	struct sunxi_rdmadev *sdev = to_sunxi_dmadev(chan->device);
+	unsigned long flags;
+
+	spin_lock_irqsave(&schan->vc.lock, flags);
+	if (vchan_issue_pending(&schan->vc) && !schan->desc) {
+		if (schan->cyclic){
+			sunxi_start_desc(schan);
+			goto out;
+		}
+
+		spin_lock(&sdev->lock);
+		if (list_empty(&schan->node))
+			list_add_tail(&schan->node, &sdev->pending);
+		spin_unlock(&sdev->lock);
+		tasklet_schedule(&sdev->task);
+	}
+out:
+	spin_unlock_irqrestore(&schan->vc.lock, flags);
+}
+
+static int sunxi_alloc_chan_resources(struct dma_chan *chan)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+	dev_dbg(chan2parent(chan), "%s: Now alloc chan resources!\n", __func__);
+
+	schan->cyclic = false;
+
+	return 0;
+}
+
+/*
+ * sunxi_free_chan_resources - free the resources of channel
+ * @chan: the channel to free
+ */
+static void sunxi_free_chan_resources(struct dma_chan *chan)
+{
+	struct sunxi_chan *schan = to_sunxi_chan(chan);
+
+	vchan_free_chan_resources(&schan->vc);
+
+	dev_dbg(chan2parent(chan), "%s: Now free chan resources!!\n", __func__);
+}
+
+/*
+ * sunxi_chan_free - free the channle on dmadevice
+ * @sdev: the dmadevice of sunxi
+ */
+static inline void sunxi_chan_free(struct sunxi_rdmadev *sdev)
+{
+	struct sunxi_chan *ch;
+
+	tasklet_kill(&sdev->task);
+	while(!list_empty(&sdev->dma_dev.channels)) {
+		ch = list_first_entry(&sdev->dma_dev.channels,
+				struct sunxi_chan, vc.chan.device_node);
+		list_del(&ch->vc.chan.device_node);
+		tasklet_kill(&ch->vc.task);
+		kfree(ch);
+	}
+
+}
+
+static int sunxi_rdma_probe(struct platform_device *pdev)
+{
+	struct sunxi_rdmadev *sunxi_dev;
+	struct sunxi_chan *schan;
+	struct resource *res;
+	int irq;
+	int ret, i;
+
+	sunxi_dev = kzalloc(sizeof(struct sunxi_rdmadev), GFP_KERNEL);
+	if (!sunxi_dev)
+		return -ENOMEM;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		ret = -EINVAL;
+		goto io_err;
+	}
+
+	sunxi_dev->base = ioremap(res->start, resource_size(res));
+	if (!sunxi_dev->base) {
+		dev_err(&pdev->dev, "Remap I/O memory failed!\n");
+		ret = -ENOMEM;
+		goto io_err;
+	}
+
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0) {
+		ret = irq;
+		goto irq_err;
+	}
+
+	ret = request_irq(irq, sunxi_rdma_interrupt, IRQF_PROBE_SHARED,
+			dev_name(&pdev->dev), sunxi_dev);
+	if (ret) {
+		dev_err(&pdev->dev, "NO IRQ found!!!\n");
+		goto irq_err;
+	}
+
+	platform_set_drvdata(pdev, sunxi_dev);
+	INIT_LIST_HEAD(&sunxi_dev->pending);
+	spin_lock_init(&sunxi_dev->lock);
+
+	/* Initialize dmaengine */
+	dma_cap_set(DMA_MEMCPY, sunxi_dev->dma_dev.cap_mask);
+	dma_cap_set(DMA_SLAVE, sunxi_dev->dma_dev.cap_mask);
+	dma_cap_set(DMA_CYCLIC, sunxi_dev->dma_dev.cap_mask);
+
+	INIT_LIST_HEAD(&sunxi_dev->dma_dev.channels);
+	sunxi_dev->dma_dev.device_alloc_chan_resources	= sunxi_alloc_chan_resources;
+	sunxi_dev->dma_dev.device_free_chan_resources	= sunxi_free_chan_resources;
+	sunxi_dev->dma_dev.device_tx_status		= sunxi_tx_status;
+	sunxi_dev->dma_dev.device_issue_pending		= sunxi_issue_pending;
+	sunxi_dev->dma_dev.device_prep_dma_cyclic	= sunxi_prep_rdma_cyclic;
+	sunxi_dev->dma_dev.device_prep_dma_memcpy	= sunxi_prep_rdma_memcpy;
+	sunxi_dev->dma_dev.device_control		= sunxi_rdma_control;
+
+	sunxi_dev->dma_dev.dev = &pdev->dev;
+
+	tasklet_init(&sunxi_dev->task, sunxi_rdma_tasklet, (unsigned long)sunxi_dev);
+
+	for (i = 0; i < NR_MAX_CHAN; i++){
+		schan = kzalloc(sizeof(*schan), GFP_KERNEL);
+		if (!schan){
+			dev_err(&pdev->dev, "%s: no memory for channel\n", __func__);
+			ret = -ENOMEM;
+			goto chan_err;
+		}
+		INIT_LIST_HEAD(&schan->node);
+		sunxi_dev->dma_dev.chancnt++;
+		schan->vc.desc_free = sunxi_free_desc;
+		vchan_init(&schan->vc, &sunxi_dev->dma_dev);
+	}
+
+	/* Register the sunxi-dma to dmaengine */
+	ret = dma_async_device_register(&sunxi_dev->dma_dev);
+	if (ret) {
+		dev_warn(&pdev->dev, "Failed to register DMA engine device: %d\n", ret);
+		goto chan_err;
+	}
+
+	sunxi_smc_writel(sunxi_smc_readl((void *)(0xf8001400 + 0x28)) | (1 << 16), (void *)(0xf8001400 + 0x28));
+	sunxi_smc_writel(sunxi_smc_readl((void *)(0xf8001400 + 0xB0)) | (1 << 16), (void *)(0xf8001400 + 0xB0));
+
+	return 0;
+
+chan_err:
+	sunxi_chan_free(sunxi_dev);
+	platform_set_drvdata(pdev, NULL);
+	free_irq(irq, sunxi_dev);
+irq_err:
+	iounmap(sunxi_dev->base);
+io_err:
+	kfree(sunxi_dev);
+	return ret;
+}
+
+static int sunxi_rdma_remove(struct platform_device *pdev)
+{
+	struct sunxi_rdmadev *sunxi_dev = platform_get_drvdata(pdev);
+
+	dma_async_device_unregister(&sunxi_dev->dma_dev);
+
+	sunxi_chan_free(sunxi_dev);
+
+	free_irq(platform_get_irq(pdev, 0), sunxi_dev);
+	iounmap(sunxi_dev->base);
+	kfree(sunxi_dev);
+
+	return 0;
+}
+
+static void sunxi_rdma_shutdown(struct platform_device *pdev)
+{
+}
+
+static int sunxi_suspend_noirq(struct device *dev)
+{
+	return 0;
+}
+
+static int sunxi_resume_noirq(struct device *dev)
+{
+	return 0;
+}
+
+static const struct dev_pm_ops sunxi_dev_pm_ops = {
+	.suspend_noirq = sunxi_suspend_noirq,
+	.resume_noirq = sunxi_resume_noirq,
+	.freeze_noirq = sunxi_suspend_noirq,
+	.thaw_noirq = sunxi_resume_noirq,
+	.restore_noirq = sunxi_resume_noirq,
+	.poweroff_noirq = sunxi_suspend_noirq,
+};
+
+static struct resource sunxi_rdma_reousce[] = {
+	[0] = {
+		.start = RDMA_PHYS_BASE,
+		.end = RDMA_PHYS_BASE + RDMA_CNT(NR_MAX_CHAN),
+		.flags = IORESOURCE_MEM,
+	},
+	[1] = {
+		.start = RDMA_IRQ_ID,
+		.end = RDMA_IRQ_ID,
+		.flags = IORESOURCE_IRQ,
+	},
+};
+
+u64 sunxi_rdma_mask = DMA_BIT_MASK(32);
+static struct platform_device sunxi_rdma_device = {
+	.name = "sunxi_rdmac",
+	.id = -1,
+	.resource = sunxi_rdma_reousce,
+	.num_resources = ARRAY_SIZE(sunxi_rdma_reousce),
+	.dev = {
+		.dma_mask = &sunxi_rdma_mask,
+		.coherent_dma_mask = DMA_BIT_MASK(32),
+	},
+};
+
+static struct platform_driver sunxi_rdma_driver = {
+	.probe		= sunxi_rdma_probe,
+	.remove		= sunxi_rdma_remove,
+	.shutdown	= sunxi_rdma_shutdown,
+	.driver = {
+		.name	= "sunxi_rdmac",
+		.pm	= &sunxi_dev_pm_ops,
+	},
+};
+
+bool sunxi_rdma_filter_fn(struct dma_chan *chan, void *param)
+{
+	bool ret = false;
+	if (chan->device->dev->driver == &sunxi_rdma_driver.driver) {
+		const char *p = param;
+		ret = !strcmp("sunxi_rdmac", p);
+		pr_debug("[sunxi_rdma]: sunxi_rdma_filter_fn: %s\n", p);
+	}
+	return ret;
+}
+EXPORT_SYMBOL_GPL(sunxi_rdma_filter_fn);
+
+static int __init sunxi_rdma_init(void)
+{
+	int ret;
+
+	platform_device_register(&sunxi_rdma_device);
+	ret = platform_driver_register(&sunxi_rdma_driver);
+
+	return ret;
+}
+subsys_initcall_sync(sunxi_rdma_init);
+
+static void __exit sunxi_rdma_exit(void)
+{
+	platform_driver_unregister(&sunxi_rdma_driver);
+	platform_device_unregister(&sunxi_rdma_device);
+}
+module_exit(sunxi_rdma_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Sunxi RDMA Controller driver");
+MODULE_AUTHOR("Sugar");
+MODULE_ALIAS("platform:sunxi_rdmac");
diff --git a/include/linux/dma/sunxi-dma.h b/include/linux/dma/sunxi-dma.h
index 32fe631ce..2a0766ab5 100644
--- a/include/linux/dma/sunxi-dma.h
+++ b/include/linux/dma/sunxi-dma.h
@@ -21,249 +21,12 @@
 #define __SUNXI_DMA_H__
 
 #include <linux/dmaengine.h>
-
-// #if defined(CONFIG_ARCH_SUN8IW10)
-// #include "sunxi/dma-sun8iw10.h"
-// #elif defined(CONFIG_ARCH_SUN8IW11)
-// #include "sunxi/dma-sun8iw11.h"
-// #elif defined(CONFIG_ARCH_SUN8IW17)
-// #include "sunxi/dma-sun8iw17.h"
-// #elif defined(CONFIG_ARCH_SUN50IW2P1)
-// #include "sunxi/dma-sun50iw2.h"
-// #elif defined(CONFIG_ARCH_SUN50IW3P1)
-// #include "sunxi/dma-sun50iw3.h"
-// #elif defined(CONFIG_ARCH_SUN50IW6P1)
-#if 1
 #include "sunxi/dma-sun50iw6.h"
-// #elif defined(CONFIG_ARCH_SUN3IW1P1)
-// #include "sunxi/dma-sun3iw1.h"
-#else
-
-#define DRQSRC_SRAM		0
-#define DRQSRC_SDRAM		0
-
-#if !defined(CONFIG_ARCH_SUN8IW5) \
-	&& !defined(CONFIG_ARCH_SUN8IW3) \
-	&& !defined(CONFIG_ARCH_SUN8IW8) \
-	&& !defined(CONFIG_ARCH_SUN8IW9)
-#define DRQSRC_SPDIFRX		2
-#endif
-
-#if !defined(CONFIG_ARCH_SUN8IW9)
-#define DRQSRC_DAUDIO_0_RX	3
-#endif
-
-#if !defined(CONFIG_ARCH_SUN9I) \
-	&& !defined(CONFIG_ARCH_SUN8IW8)
-#define DRQSRC_DAUDIO_1_RX	4
-#define DRQSRC_NAND0		5
-#endif
-
-#define DRQSRC_UART0RX		6
-#define DRQSRC_UART1RX 		7
-#define DRQSRC_UART2RX		8
-
-#ifndef CONFIG_ARCH_SUN8IW8
-#define DRQSRC_UART3RX		9
-#define DRQSRC_UART4RX		10
-#endif
-/* #define DRQSRC_RESEVER		11 */
-/* #define DRQSRC_RESEVER		12 */
-
-#ifndef CONFIG_ARCH_SUN9I
-
-#ifdef CONFIG_ARCH_SUN8IW1
-#define DRQSRC_HDMI_DDC		13
-#define DRQSRC_HDMI_AUDIO	14
-#endif
-
-#if !defined(CONFIG_ARCH_SUN8IW6)
-#define DRQSRC_AUDIO_CODEC	15
-#if defined(CONFIG_ARCH_SUN50I)
-#define DRQSRC_CODEC		DRQSRC_AUDIO_CODEC
-#endif
-#endif
-
-#if !defined(CONFIG_ARCH_SUN8IW3) \
-	&& !defined(CONFIG_ARCH_SUN8IW6)
-#define DRQSRC_SS		16
-#if defined(CONFIG_ARCH_SUN50I)
-#define DRQSRC_CE_RX		DRQSRC_SS
-#endif
-#endif
-
-#define DRQSRC_OTG_EP1		17
-#define DRQSRC_OTG_EP2		18
-#define DRQSRC_OTG_EP3		19
-#define DRQSRC_OTG_EP4		20
-
-#if !defined(CONFIG_ARCH_SUN8IW8)
-#define DRQSRC_OTG_EP5		21
-#endif
-
-#else
-#define DRQSRC_AC97		18
-#endif
-
-#if defined(CONFIG_ARCH_SUN8IW1) \
-	|| defined(CONFIG_ARCH_SUN9I)
-#define DRQSRC_UART5RX		22
-#endif
-
-#define DRQSRC_SPI0RX		23
-
-#if !defined(CONFIG_ARCH_SUN8IW8)
-#define DRQSRC_SPI1RX		24
-#endif
-
-#if defined(CONFIG_ARCH_SUN8IW1) \
-	|| defined(CONFIG_ARCH_SUN9I)
-#define DRQSRC_SPI2RX		25
-#define DRQSRC_SPI3RX		26
-#endif
-
-#if defined(CONFIG_ARCH_SUN8IW1)
-#define DRQSRC_TP		27
-#define DRQSRC_NAND1		28
-#define DRQSRC_MTC_ACC		29
-#define DRQSRC_DIGITAL_MIC	30
-
-#elif defined(CONFIG_ARCH_SUN8IW6) \
-	|| defined(CONFIG_ARCH_SUN8IW7)
-
-#define DRQDST_TDMRX		28
-#endif
-
-
-/*
- * The destination DRQ type and port corresponding relation
- *
- */
-#define DRQDST_SRAM		0
-#define DRQDST_SDRAM		0
-
-#if !defined(CONFIG_ARCH_SUN8IW5) \
-	&& !defined(CONFIG_ARCH_SUN8IW5) \
-	&& !defined(CONFIG_ARCH_SUN8IW8)
-#define DRQDST_SPDIFTX		2
-#endif
-
-#if !defined(CONFIG_ARCH_SUN8IW9)
-#define DRQDST_DAUDIO_0_TX	3
-#endif
-
-#if !defined(CONFIG_ARCH_SUN8IW8) \
-	&& !defined(CONFIG_ARCH_SUN8IW9)
-#define DRQDST_DAUDIO_1_TX	4
-#endif
-
-#if !defined(CONFIG_ARCH_SUN9I) \
-	&& !defined(CONFIG_ARCH_SUN8IW8)
-#define DRQDST_NAND0		5
-#endif
-
-#define DRQDST_UART0TX		6
-#define DRQDST_UART1TX 		7
-#define DRQDST_UART2TX		8
-
-#if !defined(CONFIG_ARCH_SUN8IW8)
-#define DRQDST_UART3TX		9
-#define DRQDST_UART4TX		10
-#endif
-
-#if defined(CONFIG_ARCH_SUN8IW3) \
-	|| defined(CONFIG_ARCH_SUN8IW5) \
-	|| defined(CONFIG_ARCH_SUN8IW8) \
-	|| defined(CONFIG_ARCH_SUN8IW9)
-#define DRQSRC_TCON0		12
-#endif
-
-#ifndef CONFIG_ARCH_SUN9I
-
-#if defined(CONFIG_ARCH_SUN8IW1)
-#define DRQDST_HDMI_DDC		13
-#define DRQDST_HDMI_AUDIO	14
-#endif
-
-#if !defined(CONFIG_ARCH_SUN8IW6)
-#define DRQDST_AUDIO_CODEC	15
-#if defined(CONFIG_ARCH_SUN50I)
-#define DRQDST_CODEC		DRQDST_AUDIO_CODEC
-#endif
-#endif
-
-#if !defined(CONFIG_ARCH_SUN8IW3) \
-	&& !defined(CONFIG_ARCH_SUN8IW6)
-#define DRQDST_SS		16
-#if defined(CONFIG_ARCH_SUN50I)
-#define DRQDST_CE_TX		DRQDST_SS
-#endif
-#endif
-
-#define DRQDST_OTG_EP1		17
-#define DRQDST_OTG_EP2		18
-#define DRQDST_OTG_EP3		19
-#define DRQDST_OTG_EP4		20
-#if !defined(CONFIG_ARCH_SUN8IW8)
-#define DRQDST_OTG_EP5		21
-#endif
-
-#else
-#define DRQDST_CIR_TX		15
-#define DRQDST_AC97		18
-#endif
-
-#if defined(CONFIG_ARCH_SUN8IW1) \
-	|| defined(CONFIG_ARCH_SUN9I)
-#define DRQDST_UART5TX		22
-#endif
-
-#define DRQDST_SPI0TX		23
-#define DRQDST_SPI1TX		24
-
-#if defined(CONFIG_ARCH_SUN8IW1) \
-	&& defined(CONFIG_ARCH_SUN9I)
-#define DRQDST_SPI2TX		25
-#define DRQDST_SPI3TX		26
-#endif
-
-#if defined(CONFIG_ARCH_SUN8IW1)
-#define DRQDST_NAND1		28
-#define DRQDST_MTC_ACC		29
-#define DRQDST_DIGITAL_MIC	30
-
-#elif defined(CONFIG_ARCH_SUN8IW6) \
-	|| defined(CONFIG_ARCH_SUN8IW7)
-
-#define DRQDST_DAUDIO_2_TX	27
-#define DRQDST_TDM_TX		28
-#define	DRQDST_CIR_TX		29
-#elif defined(CONFIG_ARCH_SUN50I)
-#define DRQDST_DAUDIO_2_TX	27
-#endif
-
-#endif /* CONFIG_ARCH_SUN8IW10 */
 
 #define sunxi_slave_id(d, s) (((d)<<16) | (s))
 #define GET_SRC_DRQ(x)	((x) & 0x000000ff)
 #define GET_DST_DRQ(x)	((x) & 0x00ff0000)
 
-/*
- * The following DRQ type just for CPUs.
- */
-#ifdef CONFIG_ARCH_SUN9I
-#define DRQSRC_R_DAUDIO_0_RX	1
-#define DRQSRC_R_UART_RX	2
-#define DRQSRC_R_CIR_RX		3
-#define DRQSRC_R_DAUDIO_1_RX	4
-#define DRQSRC_SPI3_RX		5
-
-#define DRQDST_R_DAUDIO_0_TX	1
-#define DRQDST_R_UART_TX	2
-#define DRQDST_R_CIR_TX		3
-#define DRQDST_R_DAUDIO_1_TX	4
-#define DRQDST_SPI3_TX		5
-#endif
 
 #define SUNXI_DMA_DRV	"sunxi_dmac"
 #define SUNXI_RDMA_DRV	"sunxi_rdmac"
diff --git a/include/linux/sunxi-sid.h b/include/linux/sunxi-sid.h
new file mode 100644
index 000000000..5d187236e
--- /dev/null
+++ b/include/linux/sunxi-sid.h
@@ -0,0 +1,125 @@
+/*
+ * linux/sunxi-sid.h
+ *
+ * Copyright(c) 2014-2016 Allwinnertech Co., Ltd.
+ *         http://www.allwinnertech.com
+ *
+ * Author: sunny <sunny@allwinnertech.com>
+ *
+ * allwinner sunxi soc chip version and chip id manager.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#ifndef __SUNXI_MACH_SUNXI_CHIP_H
+#define __SUNXI_MACH_SUNXI_CHIP_H
+
+#include <linux/types.h>
+
+/* About ChipID of version */
+
+#define SUNXI_CHIP_REV(p, v)  (p + v)
+
+#define SUNXI_CHIP_SUN8IW11   (0x17010000)
+#define SUN8IW11P1_REV_A SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW11, 0x0000)
+#define SUN8IW11P2_REV_A SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW11, 0x0001)
+#define SUN8IW11P3_REV_A SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW11, 0x0011)
+#define SUN8IW11P4_REV_A SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW11, 0x0101)
+
+#define SUNXI_CHIP_SUN8IW10   (0x16990000)
+#define SUN8IW10P1_REV_B SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW10, 0x1001)
+#define SUN8IW10P2_REV_B SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW10, 0x1000)
+#define SUN8IW10P3_REV_B SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW10, 0x1003)
+#define SUN8IW10P4_REV_B SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW10, 0x1002)
+#define SUN8IW10P5_REV_B SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW10, 0x100B)
+#define SUN8IW10P6_REV_B SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW10, 0x100A)
+#define SUN8IW10P7_REV_B SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW10, 0x1007)
+#define SUN8IW10P8_REV_B SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW10, 0x1006)
+
+#define SUNXI_CHIP_SUN8IW17   (0x17080000)
+#define SUN8IW17P1_REV_A SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW17, 0x0000)
+
+#define SUNXI_CHIP_SUN8IW5 (0x16670000)
+#define SUN8IW5P1_REV_A SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW5, 0x0000)
+#define SUN8IW5P1_REV_B SUNXI_CHIP_REV(SUNXI_CHIP_SUN8IW5, 0x0001)
+
+#define SUNXI_CHIP_SUN50IW1   (0x16890000)
+#define SUN50IW1P1_REV_A	SUNXI_CHIP_REV(SUNXI_CHIP_SUN50IW1, 0x0)
+
+#define SUNXI_CHIP_SUN50IW2   (0x17180000)
+#define SUN50IW2P1_REV_A	SUNXI_CHIP_REV(SUNXI_CHIP_SUN50IW2, 0x0)
+
+#define SUNXI_CHIP_SUN50IW3   (0x17190000)
+#define SUN50IW3P1_REV_A	SUNXI_CHIP_REV(SUNXI_CHIP_SUN50IW3, 0x0)
+
+#define SUNXI_CHIP_SUN50IW6   (0x17280000)
+#define SUN50IW6P1_REV_A	SUNXI_CHIP_REV(SUNXI_CHIP_SUN50IW6, 0x0)
+
+/* The key info in Efuse */
+
+#define EFUSE_CHIPID_NAME            "chipid"
+#define EFUSE_BROM_CONF_NAME         "brom_conf"
+#define EFUSE_BROM_TRY_NAME          "brom_try"
+#define EFUSE_THM_SENSOR_NAME        "thermal_sensor"
+#define EFUSE_FT_ZONE_NAME           "ft_zone"
+#define EFUSE_TV_OUT_NAME            "tvout"
+#define EFUSE_OEM_NAME               "oem"
+
+#define EFUSE_WR_PROTECT_NAME        "write_protect"
+#define EFUSE_RD_PROTECT_NAME        "read_protect"
+#define EFUSE_IN_NAME                "in"
+#define EFUSE_ID_NAME                "id"
+#define EFUSE_ROTPK_NAME             "rotpk"
+#define EFUSE_SSK_NAME               "ssk"
+#define EFUSE_RSSK_NAME              "rssk"
+#define EFUSE_HDCP_HASH_NAME         "hdcp_hash"
+#define EFUSE_HDCP_PKF_NAME          "hdcp_pkf"
+#define EFUSE_HDCP_DUK_NAME          "hdcp_duk"
+#define EFUSE_EK_HASH_NAME           "ek_hash"
+#define EFUSE_SN_NAME                "sn"
+#define EFUSE_NV1_NAME               "nv1"
+#define EFUSE_NV2_NAME               "nv2"
+#define EFUSE_BACKUP_KEY_NAME        "backup_key"
+#define EFUSE_RSAKEY_HASH_NAME       "rsakey_hash"
+#define EFUSE_RENEW_NAME             "renewability"
+#define EFUSE_OPT_ID_NAME            "operator_id"
+#define EFUSE_LIFE_CYCLE_NAME        "life_cycle"
+#define EFUSE_JTAG_SECU_NAME         "jtag_security"
+#define EFUSE_JTAG_ATTR_NAME         "jtag_attr"
+#define EFUSE_CHIP_CONF_NAME         "chip_config"
+#define EFUSE_RESERVED_NAME          "reserved"
+#define EFUSE_RESERVED2_NAME         "reserved2"
+/* For KeyLadder */
+#define EFUSE_KL_SCK0_NAME           "keyladder_sck0"
+#define EFUSE_KL_KEY0_NAME           "keyladder_master_key0"
+#define EFUSE_KL_SCK1_NAME           "keyladder_sck1"
+#define EFUSE_KL_KEY1_NAME           "keyladder_master_key1"
+
+#define SUNXI_KEY_NAME_LEN	32
+
+#define EFUSE_CHIPID_BASE	"allwinner,sunxi-chipid"
+#define EFUSE_SID_BASE		"allwinner,sunxi-sid"
+
+#define sunxi_efuse_read(key_name, read_buf) \
+		sunxi_efuse_readn(key_name, read_buf, 1024)
+
+/* The interface functions */
+
+unsigned int sunxi_get_soc_ver(void);
+int sunxi_get_soc_chipid(u8 *chipid);
+int sunxi_get_soc_chipid_str(char *chipid);
+int sunxi_get_pmu_chipid(u8 *chipid);
+int sunxi_get_serial(u8 *serial);
+unsigned int sunxi_get_soc_bin(void);
+int sunxi_soc_is_secure(void);
+s32 sunxi_get_platform(s8 *buf, s32 size);
+s32 sunxi_efuse_readn(void *key_name, void *buf, u32 n);
+s32 sunxi_efuse_readn_ext(void *key_name, void *buf, u32 n);
+unsigned int sunxi_get_soc_customerid(void);
+int sunxi_check_soc_rotpk(void *rotpk, int buf_len);
+s32 sunxi_get_soc_rotpk(void *rotpk, int buf_len);
+s32 sunxi_get_soc_sn(void *sn, int buf_len);
+#endif  /* __SUNXI_MACH_SUNXI_CHIP_H */
diff --git a/include/linux/sunxi-smc.h b/include/linux/sunxi-smc.h
new file mode 100644
index 000000000..203dfd3f3
--- /dev/null
+++ b/include/linux/sunxi-smc.h
@@ -0,0 +1,46 @@
+/*
+ * drivers/char/sunxi_sys_info/sunxi-smc.c
+ *
+ * Copyright(c) 2015-2016 Allwinnertech Co., Ltd.
+ *         http://www.allwinnertech.com
+ *
+ * Author: sunny <superm@allwinnertech.com>
+ *
+ * allwinner sunxi soc chip version and chip id manager.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#ifndef __SUNXI_SMC_H
+#define __SUNXI_SMC_H
+
+extern int invoke_smc_fn(u32 function_id, u64 arg0, u64 arg1, u64 arg2);
+extern int sunxi_smc_readl(phys_addr_t addr);
+extern int sunxi_smc_writel(u32 value, phys_addr_t addr);
+extern int sunxi_smc_probe_secure(void);
+extern int sunxi_soc_is_secure(void);
+
+static inline u32 sunxi_readl(phys_addr_t addr)
+{
+#ifdef CONFIG_SUNXI_SMC
+	if (sunxi_soc_is_secure())
+		return sunxi_smc_readl(addr);
+	else
+#endif
+	return readl(phys_to_virt(addr));
+}
+
+static inline void sunxi_writel(u32 val, phys_addr_t addr)
+{
+#ifdef CONFIG_SUNXI_SMC
+	if (sunxi_soc_is_secure())
+		sunxi_smc_writel(val, addr);
+	else
+#endif
+	writel(val, phys_to_virt(addr));
+}
+
+#endif  /* __SUNXI_SMC_H */
diff --git a/include/linux/sunxi_mbus.h b/include/linux/sunxi_mbus.h
new file mode 100644
index 000000000..56d07e5d1
--- /dev/null
+++ b/include/linux/sunxi_mbus.h
@@ -0,0 +1,132 @@
+/*
+ * SUNXI MBUS support
+ *
+ * Copyright (C) 2015 AllWinnertech Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ */
+
+#ifndef __LINUX_SUNXI_MBUS_H
+#define __LINUX_SUNXI_MBUS_H
+
+#include <linux/types.h>
+
+/* Port ids */
+typedef enum mbus_port {
+	MBUS_PORT_CPU           = 0,
+	MBUS_PORT_GPU           = 1,
+	MBUS_PORT_MAHB          = 2,
+	MBUS_PORT_DMA           = 3,
+	MBUS_PORT_VE            = 4,
+	MBUS_PORT_CE            = 5,
+	MBUS_PORT_TSC0          = 6,
+	MBUS_PORT_NDFC0         = 8,
+	MBUS_PORT_CSI0          = 11,
+	MBUS_PORT_DI0           = 14,
+	MBUS_PORT_DI1           = 15,
+	MBUS_PORT_DE300         = 16,
+	MBUS_PORT_IOMMU         = 25,
+	MBUS_PORT_VE2           = 26,
+	MBUS_PORT_USB3_0        = 37,
+	MBUS_PORT_PCIE          = 38,
+	MBUS_PORT_VP9           = 39,
+	MBUS_PORT_HDCP2_0       = 40,
+	MBUS_PORTS_MAX          = 41,
+} mbus_port_e;
+
+struct device_node;
+
+#ifdef CONFIG_SUNXI_MBUS
+extern int mbus_port_setbwlen(mbus_port_e port, bool en);
+extern int mbus_port_setpri(mbus_port_e port, bool pri);
+extern int mbus_port_setqos(mbus_port_e port, unsigned int qos);
+extern int mbus_port_setwt(mbus_port_e port, unsigned int wt);
+extern int mbus_port_setacs(mbus_port_e port, unsigned int acs);
+extern int mbus_port_setbwl0(mbus_port_e port, unsigned int bwl0);
+extern int mbus_port_setbwl1(mbus_port_e port, unsigned int bwl1);
+extern int mbus_port_setbwl2(mbus_port_e port, unsigned int bwl2);
+extern int mbus_port_setbwl(mbus_port_e port, unsigned int bwl0, \
+                             unsigned int bwl1, unsigned int bwl2);
+extern int mbus_set_bwlwen(bool enable);
+extern int mbus_set_bwlwsiz(unsigned int size);
+extern int mbus_port_control_by_index(mbus_port_e port, bool enable);
+extern bool mbus_probed(void);
+extern int mbus_port_setbwcu(unsigned int unit);
+#else
+static inline int mbus_port_setbwlen(mbus_port_e port, bool en)
+{
+	return -ENODEV;
+}
+static inline int mbus_port_setpri(mbus_port_e port, bool pri)
+{
+	return -ENODEV;
+}
+static inline int mbus_port_setqos(mbus_port_e port, unsigned int qos)
+{
+	return -ENODEV;
+}
+static inline int mbus_port_setwt(mbus_port_e port, unsigned int wt)
+{
+	return -ENODEV;
+}
+static inline int mbus_port_setacs(mbus_port_e port, unsigned int acs)
+{
+	return -ENODEV;
+}
+static inline int mbus_port_setbwl0(mbus_port_e port, unsigned int bwl0)
+{
+	return -ENODEV;
+}
+static inline int mbus_port_setbwl1(mbus_port_e port, unsigned int bwl1)
+{
+	return -ENODEV;
+}
+static inline int mbus_port_setbwl2(mbus_port_e port, unsigned int bwl2)
+{
+	return -ENODEV;
+}
+static inline int mbus_port_setbwl(mbus_port_e port, unsigned int bwl0, \
+                                   unsigned int bwl1, unsigned int bwl2)
+{
+	return -ENODEV;
+}
+static inline int mbus_set_bwlwen(bool enable)
+{
+	return -ENODEV;
+}
+static inline int mbus_set_bwlwsiz(unsigned int size)
+{
+	return -ENODEV;
+}
+static inline int mbus_port_control_by_index(mbus_port_e port, bool enable)
+{
+	return -ENODEV;
+}
+static inline bool mbus_probed(void)
+{
+	return false;
+}
+static inline int mbus_port_setbwcu(unsigned int unit)
+{
+	return -ENODEV;
+}
+#endif
+
+#define mbus_disable_port_by_index(dev) \
+	mbus_port_control_by_index(dev, false)
+#define mbus_enable_port_by_index(dev) \
+	mbus_port_control_by_index(dev, true)
+
+#endif
-- 
2.17.1

